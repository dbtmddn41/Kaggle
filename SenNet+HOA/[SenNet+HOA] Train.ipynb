{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":61446,"databundleVersionId":6962461,"sourceType":"competition"},{"sourceId":1807973,"sourceType":"datasetVersion","datasetId":1074109},{"sourceId":6921119,"sourceType":"datasetVersion","datasetId":3971791},{"sourceId":150248402,"sourceType":"kernelVersion"}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# FPN\n# 6 version of 2.5d Cutting model baseline [training]\n#  author YOYOBAR\nhttps://www.kaggle.com/code/yoyobar/2-5d-cutting-model-baseline-training","metadata":{}},{"cell_type":"markdown","source":"**This code is base on [2.5d segmentaion baseline [training]](https://www.kaggle.com/code/tanakar/2-5d-segmentaion-baseline-training)**\nIf you think my code is useful,please upvote it ^w^.\n* Version6:\n1. *     using kidney_1_dense for training and kidney_3_dense for val\n2. *     image_size = 512\n3. *     useing DiceLoss\n4. *     norm_with_clip\n5. *     fix some bug\n\n\n* This version is correspond with [2.5d Cutting model baseline [inference]](https://www.kaggle.com/code/yoyobar/2-5d-cutting-model-baseline-inference) version3\n\n\n\nAccording to my experiments, using kidney_1_dense for training and kidney_3_dense for val is the best. You can even get 0.757, but using 2d model(se_resnext50_32x4d), you can set CFG.in_chans=1 to make this notebook as a 2d model training notebook.'","metadata":{}},{"cell_type":"markdown","source":"# Import","metadata":{}},{"cell_type":"code","source":"!mkdir -p /root/.cache/torch/hub/checkpoints/\n!cp /kaggle/input/se-net-pretrained-imagenet-weights/* /root/.cache/torch/hub/checkpoints/\nimport torch as tc \nimport torch.nn as nn  \nimport numpy as np\nfrom tqdm import tqdm\nimport os,sys,cv2\nfrom torch.cuda.amp import autocast\nimport matplotlib.pyplot as plt\nimport albumentations as A\n!python -m pip install --no-index --find-links=/kaggle/input/pip-download-for-segmentation-models-pytorch segmentation-models-pytorch\nimport segmentation_models_pytorch as smp\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.parallel import DataParallel\nfrom glob import glob\nimport pickle","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-02-02T20:56:09.025142Z","iopub.execute_input":"2024-02-02T20:56:09.025548Z","iopub.status.idle":"2024-02-02T20:56:55.786436Z","shell.execute_reply.started":"2024-02-02T20:56:09.025519Z","shell.execute_reply":"2024-02-02T20:56:55.785605Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"Looking in links: /kaggle/input/pip-download-for-segmentation-models-pytorch\nProcessing /kaggle/input/pip-download-for-segmentation-models-pytorch/segmentation_models_pytorch-0.3.3-py3-none-any.whl\nRequirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (0.15.1)\nProcessing /kaggle/input/pip-download-for-segmentation-models-pytorch/pretrainedmodels-0.7.4.tar.gz (from segmentation-models-pytorch)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hProcessing /kaggle/input/pip-download-for-segmentation-models-pytorch/efficientnet_pytorch-0.7.1.tar.gz (from segmentation-models-pytorch)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hProcessing /kaggle/input/pip-download-for-segmentation-models-pytorch/timm-0.9.2-py3-none-any.whl (from segmentation-models-pytorch)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (4.66.1)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (10.1.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.0.0)\nProcessing /kaggle/input/pip-download-for-segmentation-models-pytorch/munch-4.0.0-py2.py3-none-any.whl (from pretrainedmodels==0.7.4->segmentation-models-pytorch)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation-models-pytorch) (6.0.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation-models-pytorch) (0.17.3)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->segmentation-models-pytorch) (0.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.24.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (2.31.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2023.10.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (21.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2023.7.22)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\nBuilding wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16428 sha256=d8f9d98c99a94ba9b45e150c52a3a7902ac2461e9424f9e0db2fd5fb12836025\n  Stored in directory: /root/.cache/pip/wheels/d2/e7/71/a031831a75a14914d29e2f255fcbc113d825ff4762d42f0315\n  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60943 sha256=244dae47519917ec370baf0d1a56b805dc2cd3268e86cee1d0f59c1d40eff96f\n  Stored in directory: /root/.cache/pip/wheels/10/53/cc/d9cbdaa15d821ad4845e69994708dcad78fcddf4c92a753bdf\nSuccessfully built efficientnet-pytorch pretrainedmodels\nInstalling collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation-models-pytorch\n  Attempting uninstall: timm\n    Found existing installation: timm 0.9.10\n    Uninstalling timm-0.9.10:\n      Successfully uninstalled timm-0.9.10\nSuccessfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.3 timm-0.9.2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# config","metadata":{}},{"cell_type":"code","source":"p_augm = 0.05 #0.5\n#add rotate.  less p_augm\n\nclass CFG:\n    # ============== pred target =============\n    target_size = 1\n\n    # ============== model CFG =============\n    model_name = 'Unet'\n    backbone = 'tu-maxxvitv2_nano_rw_256'\n\n    in_chans = 1   #5 # 65\n    # ============== training CFG =============\n    image_size = 1024 # 512 # 512\n    input_size = 1024 # 512 #=512\n\n    train_batch_size = 8 #4 #16\n    valid_batch_size = train_batch_size * 2\n\n    epochs = 35 #27 #30 #25\n    lr = 9e-5\n    chopping_percentile=1e-3\n    # ============== fold =============\n    valid_id = 1\n\n\n    # ============== augmentation =============\n    train_aug_list = [\n        A.Rotate(limit=270, p= 0.5),\n        A.RandomScale(scale_limit=(0.8,1.25),interpolation=cv2.INTER_CUBIC,p=p_augm),\n        A.RandomCrop(input_size, input_size,p=1),\n        A.RandomGamma(p=p_augm*2/3),\n        A.RandomBrightnessContrast(p=p_augm,),\n        A.GaussianBlur(p=p_augm),\n        A.MotionBlur(p=p_augm),\n        A.GridDistortion(num_steps=5, distort_limit=0.3, p=p_augm),\n        ToTensorV2(transpose_mask=True),\n    ]\n    train_aug = A.Compose(train_aug_list, is_check_shapes=False)\n    valid_aug_list = [\n        ToTensorV2(transpose_mask=True),\n    ]\n    valid_aug = A.Compose(valid_aug_list)","metadata":{"execution":{"iopub.status.busy":"2024-02-02T21:16:31.342741Z","iopub.execute_input":"2024-02-02T21:16:31.343162Z","iopub.status.idle":"2024-02-02T21:16:31.353466Z","shell.execute_reply.started":"2024-02-02T21:16:31.343129Z","shell.execute_reply":"2024-02-02T21:16:31.352419Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class CustomModel(nn.Module):\n    def __init__(self, CFG, weight=None):\n        super().__init__()\n        self.model = smp.Unet(  #DeepLabV3+\n            encoder_name=CFG.backbone, \n            encoder_weights=weight,\n            in_channels=CFG.in_chans,\n            classes=CFG.target_size,\n            activation=None,\n        )\n\n    def forward(self, image):\n        output = self.model(image)\n        # output = output.squeeze(-1)\n        return output[:,0]#.sigmoid()\n\n\ndef build_model(weight=\"imagenet\"):\n    from dotenv import load_dotenv\n    load_dotenv()\n\n    print('model_name', CFG.model_name)\n    print('backbone', CFG.backbone)\n\n    model = CustomModel(CFG, weight)\n\n    return model.cuda()","metadata":{"execution":{"iopub.status.busy":"2024-02-02T20:56:55.799947Z","iopub.execute_input":"2024-02-02T20:56:55.800281Z","iopub.status.idle":"2024-02-02T20:56:55.812740Z","shell.execute_reply.started":"2024-02-02T20:56:55.800250Z","shell.execute_reply":"2024-02-02T20:56:55.811949Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Change size","metadata":{}},{"cell_type":"code","source":"\ndef to_1024(img , image_size = 1024):\n    if image_size > img.shape[1]:\n       img = np.rot90(img)\n       start1 = (CFG.image_size - img.shape[0])//2 \n       top =     img[0                    : start1,   0: img.shape[1] ]\n       bottom  = img[img.shape[0] -start1 : img.shape[0],   0 : img.shape[1] ]\n       img_result = np.concatenate((top,img,bottom ),axis=0)\n       img_result = np.rot90(img_result)\n       img_result = np.rot90(img_result)\n       img_result = np.rot90(img_result)\n    else :\n       img_result = img\n    return img_result\n\ndef to_1024_no_rot(img, image_size = 1024):\n    if image_size > img.shape[0]:  \n       start1 = ( image_size - img.shape[0])//2\n       top =     img[0                    : start1,   0: img.shape[1] ]\n       bottom  = img[img.shape[0] -start1 : img.shape[0],   0 : img.shape[1] ]\n       img_result = np.concatenate((top,img,bottom ),axis=0)\n    else: \n       img_result = img\n    return img_result\n\n#  add border\ndef to_1024_1024(img  , image_size = 1024 ):\n     img_result = to_1024(img, image_size )\n     return img_result\n    \n#  drop border\ndef to_original ( im_after, img, image_size = 1024 ):\n    top_ = 0\n    left_ = 0\n    if (im_after.shape[0] > img.shape[0]):\n             top_  = ( image_size - img.shape[0])//2 \n    if    (im_after.shape[1] > img.shape[1]) :\n             left_  = ( image_size - img.shape[1])//2  \n    if (top_>0)or (left_>0) :\n             img_result = im_after[top_                    : img.shape[0] + top_,   left_: img.shape[1] + left_ ]\n             #print(im_after.shape,'-->',img_result.shape)\n    else:\n             img_result = im_after\n    return img_result  ","metadata":{"execution":{"iopub.status.busy":"2024-02-02T20:56:55.813903Z","iopub.execute_input":"2024-02-02T20:56:55.814644Z","iopub.status.idle":"2024-02-02T20:56:55.827975Z","shell.execute_reply.started":"2024-02-02T20:56:55.814611Z","shell.execute_reply":"2024-02-02T20:56:55.827163Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Functions","metadata":{}},{"cell_type":"code","source":"def min_max_normalization(x:tc.Tensor)->tc.Tensor:\n    \"\"\"input.shape=(batch,f1,...)\"\"\"\n    shape=x.shape\n    if x.ndim>2:\n        x=x.reshape(x.shape[0],-1)\n    \n    min_=x.min(dim=-1,keepdim=True)[0]\n    max_=x.max(dim=-1,keepdim=True)[0]\n    if min_.mean()==0 and max_.mean()==1:\n        return x.reshape(shape)\n    \n    x=(x-min_)/(max_-min_+1e-9)\n    return x.reshape(shape)\n\ndef norm_with_clip(x:tc.Tensor,smooth=1e-5):\n    dim=list(range(1,x.ndim))\n    mean=x.mean(dim=dim,keepdim=True)\n    std=x.std(dim=dim,keepdim=True)\n    x=(x-mean)/(std+smooth)\n    x[x>5]=(x[x>5]-5)*1e-3 +5\n    x[x<-3]=(x[x<-3]+3)*1e-3-3\n    return x\n\ndef add_noise(x:tc.Tensor,max_randn_rate=0.1,randn_rate=None,x_already_normed=False):\n    \"\"\"input.shape=(batch,f1,f2,...) output's var will be normalizate  \"\"\"\n    ndim=x.ndim-1\n    if x_already_normed:\n        x_std=tc.ones([x.shape[0]]+[1]*ndim,device=x.device,dtype=x.dtype)\n        x_mean=tc.zeros([x.shape[0]]+[1]*ndim,device=x.device,dtype=x.dtype)\n    else: \n        dim=list(range(1,x.ndim))\n        x_std=x.std(dim=dim,keepdim=True)\n        x_mean=x.mean(dim=dim,keepdim=True)\n    if randn_rate is None:\n        randn_rate=max_randn_rate*np.random.rand()*tc.rand(x_mean.shape,device=x.device,dtype=x.dtype)\n    cache=(x_std**2+(x_std*randn_rate)**2)**0.5\n    #https://blog.csdn.net/chaosir1991/article/details/106960408\n    \n    return (x-x_mean+tc.randn(size=x.shape,device=x.device,dtype=x.dtype)*randn_rate*x_std)/(cache+1e-7)\n \nclass Data_loader(Dataset):\n     \n    def __init__(self,paths,is_label):\n        self.paths=paths\n        self.paths.sort()\n        self.is_label=is_label\n    \n    def __len__(self):\n        return len(self.paths)\n    \n    def __getitem__(self,index):\n         \n        img = cv2.imread(self.paths[index],cv2.IMREAD_GRAYSCALE)\n        \n        img = to_1024_1024(img , image_size = CFG.image_size ) #  to_original( im_after, img_save, image_size = 1024)\n\n        img = tc.from_numpy(img.copy())\n        if self.is_label:\n            img=(img!=0).to(tc.uint8)*255\n        else:\n            img=img.to(tc.uint8)\n        return img\n\ndef load_data(paths,is_label=False):\n    data_loader=Data_loader(paths,is_label)\n    data_loader=DataLoader(data_loader, batch_size=16, num_workers=2)  \n    data=[]\n    for x in tqdm(data_loader):\n        data.append(x)\n    x=tc.cat(data,dim=0)\n    del data\n    if not is_label:\n        ########################################################################\n        TH=x.reshape(-1).numpy()\n        index = -int(len(TH) * CFG.chopping_percentile)\n        TH:int = np.partition(TH, index)[index]\n        x[x>TH]=int(TH)\n        ########################################################################\n        TH=x.reshape(-1).numpy()\n        index = -int(len(TH) * CFG.chopping_percentile)\n        TH:int = np.partition(TH, -index)[-index]\n        x[x<TH]=int(TH)\n        ########################################################################\n        x=(min_max_normalization(x.to(tc.float16)[None])[0]*255).to(tc.uint8)\n    return x\n\n\n#https://www.kaggle.com/code/kashiwaba/sennet-hoa-train-unet-simple-baseline\ndef dice_coef(y_pred:tc.Tensor,y_true:tc.Tensor, thr=0.5, dim=(-1,-2), epsilon=0.001):\n    y_pred=y_pred.sigmoid()\n    y_true = y_true.to(tc.float32)\n    y_pred = (y_pred>thr).to(tc.float32)\n    inter = (y_true*y_pred).sum(dim=dim)\n    den = y_true.sum(dim=dim) + y_pred.sum(dim=dim)\n    dice = ((2*inter+epsilon)/(den+epsilon)).mean()\n    return dice\n\nclass DiceLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceLoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n        inputs = inputs.sigmoid()   \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        intersection = (inputs * targets).sum()                            \n        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n        \n        return 1 - dice\n\nclass Kaggld_Dataset(Dataset):\n    def __init__(self,x:list,y:list,arg=False):\n        super(Dataset,self).__init__()\n        self.x=x#list[(C,H,W),...]\n        self.y=y#list[(C,H,W),...]\n        self.image_size=CFG.image_size\n        self.in_chans=CFG.in_chans\n        self.arg=arg\n        if arg:\n            self.transform=CFG.train_aug\n        else: \n            self.transform=CFG.valid_aug\n\n    def __len__(self) -> int:\n        return sum([y.shape[0]-self.in_chans for y in self.y])\n    \n    def __getitem__(self,index):\n        i=0\n        for x in self.x:\n            if index>x.shape[0]-self.in_chans:\n                index-=x.shape[0]-self.in_chans\n                i+=1\n            else:\n                break\n        x=self.x[i]\n        y=self.y[i]\n        \n#         print(f'x.shape[1] ={x.shape[1]}    x.shape[2]={x.shape[2]}')\n        x_index= (x.shape[1]-self.image_size)//2 #np.random.randint(0,x.shape[1]-self.image_size)\n        y_index= (x.shape[2]-self.image_size)//2 # np.random.randint(0,x.shape[2]-self.image_size)\n        # i i+5 \n        x=x[index:index+self.in_chans   ,   x_index:x_index+self.image_size,   y_index:y_index+self.image_size]\n        # i+2\n        y=y[index+self.in_chans//2   ,      x_index:x_index+self.image_size,   y_index:y_index+self.image_size]\n        data = self.transform(image=x.numpy().transpose(1,2,0), mask=y.numpy())\n        x = data['image']\n        y = data['mask']>=127\n        if self.arg:\n            i=np.random.randint(4)\n            x=x.rot90(i,dims=(1,2))\n            y=y.rot90(i,dims=(0,1))\n            for i in range(3):\n                if np.random.randint(2):\n                    x=x.flip(dims=(i,))\n                    if i>=1:\n                        y=y.flip(dims=(i-1,))\n        return x,y#(uint8,uint8)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-02T20:56:55.830818Z","iopub.execute_input":"2024-02-02T20:56:55.831143Z","iopub.status.idle":"2024-02-02T20:56:55.865886Z","shell.execute_reply.started":"2024-02-02T20:56:55.831113Z","shell.execute_reply":"2024-02-02T20:56:55.865110Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def np_metric(predict, truth):\n    p = (predict>0.5)\n    t = (truth>0.5)\n    hit = (p*t).sum()\n    fp  = (p*(1-t)).sum()\n    t_sum = t.sum()\n    p_sum = p.sum()\n    return hit/t_sum, fp/p_sum","metadata":{"execution":{"iopub.status.busy":"2024-02-02T20:56:55.866987Z","iopub.execute_input":"2024-02-02T20:56:55.867312Z","iopub.status.idle":"2024-02-02T20:56:55.880425Z","shell.execute_reply.started":"2024-02-02T20:56:55.867282Z","shell.execute_reply":"2024-02-02T20:56:55.879590Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Load data ","metadata":{}},{"cell_type":"code","source":"path1=\"/kaggle/input/sennet-hoa-png/train_png_normalized/kidney_3_sparse\"\npath2=\"/kaggle/input/sennet-hoa-png/train_png_normalized/kidney_2\"\npaths_y=glob(f\"{path2}/labels/*\")\npaths_x=[x.replace(\"labels\",\"images\") for x in paths_y]\n\nval_x=load_data(paths_x,is_label=False)\nprint(val_x.shape)\nval_y=load_data(paths_y,is_label=True)\nprint(val_y.shape)\nwith open('val.pkl', 'wb') as f:\n    pickle.dump((val_x, val_y), f)\ndel val_x, val_y\n\ntrain_x=[]\ntrain_y=[]\n\nroot_path=\"/kaggle/input/blood-vessel-segmentation/\"\nparhs=[\"/kaggle/input/sennet-hoa-png/train_png_normalized/kidney_3_dense\", \"/kaggle/input/sennet-hoa-png/train_png_normalized/kidney_1_dense\", ]\nfor i,path in enumerate(parhs):\n    y=load_data(glob(f\"{path}/labels/*\"),is_label=True)\n    if path==\"/kaggle/input/sennet-hoa-png/train_png_normalized/kidney_3_dense\":\n        paths_x=[x.replace(\"labels\",\"images\").replace(\"dense\",\"sparse\") for x in glob(f\"{path}/labels/*\")]\n        x=load_data(paths_x,is_label=False)\n    else:\n        x=load_data(glob(f\"{path}/images/*\"),is_label=False)\n    print(x.shape)\n    \n    print(y.shape)\n    train_x.append(x)\n    train_y.append(y)\n\n    #(C,H,W)\n\n    #aug\n    if path!=\"/kaggle/input/sennet-hoa-png/train_png_normalized/kidney_3_dense\":\n        train_x.append(x.permute(1,2,0))\n        train_y.append(y.permute(1,2,0))\n        train_x.append(x.permute(2,0,1))\n        train_y.append(y.permute(2,0,1))\n\nwith open('val.pkl', 'rb') as f:\n    val_x, val_y = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2024-02-02T21:02:15.272786Z","iopub.execute_input":"2024-02-02T21:02:15.273190Z","iopub.status.idle":"2024-02-02T21:09:38.164509Z","shell.execute_reply.started":"2024-02-02T21:02:15.273157Z","shell.execute_reply":"2024-02-02T21:09:38.163162Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"100%|██████████| 139/139 [00:44<00:00,  3.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([2217, 1041, 1511])\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 139/139 [00:22<00:00,  6.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([2217, 1041, 1511])\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 143/143 [00:35<00:00,  4.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([2279, 1303, 1024])\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 143/143 [00:21<00:00,  6.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([2279, 1303, 1024])\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 65/65 [00:19<00:00,  3.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1035, 1706, 1510])\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 32/32 [00:06<00:00,  4.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([501, 1706, 1510])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"import ssl\n\nssl._create_default_https_context = ssl._create_unverified_context","metadata":{"execution":{"iopub.status.busy":"2024-02-02T21:09:38.167012Z","iopub.execute_input":"2024-02-02T21:09:38.167336Z","iopub.status.idle":"2024-02-02T21:09:38.172560Z","shell.execute_reply.started":"2024-02-02T21:09:38.167309Z","shell.execute_reply":"2024-02-02T21:09:38.171521Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"tc.backends.cudnn.enabled = True\ntc.backends.cudnn.benchmark = True\n    \ntrain_dataset=Kaggld_Dataset(train_x,train_y,arg=True)\ntrain_dataset = DataLoader(train_dataset, batch_size=CFG.train_batch_size ,num_workers=2, shuffle=True, pin_memory=True, drop_last=True)\nval_dataset=Kaggld_Dataset([val_x],[val_y])\nval_dataset = DataLoader(val_dataset, batch_size=CFG.valid_batch_size, num_workers=2, shuffle=False, pin_memory=True, drop_last=True)\n\nmodel=build_model()\nmodel=DataParallel(model)\n\nloss_fc=DiceLoss()\n#loss_fn=nn.BCEWithLogitsLoss()\noptimizer=tc.optim.AdamW(model.parameters(),lr=CFG.lr)\nscaler=tc.cuda.amp.GradScaler()\nscheduler = tc.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=CFG.lr,\n                                                steps_per_epoch=len(train_dataset), epochs=CFG.epochs+1,\n                                                pct_start=0.1,)\nbest_score = 0.\nmargin=0.01\nfor epoch in range(CFG.epochs):\n    model.train()\n    time=tqdm(range(len(train_dataset)))\n    losss=0\n    scores=0\n    for i,(x,y) in enumerate(train_dataset):\n        x=x.cuda().to(tc.float32)\n        y=y.cuda().to(tc.float32)\n        x=norm_with_clip(x.reshape(-1,*x.shape[2:])).reshape(x.shape)\n        x=add_noise(x,max_randn_rate=0.5,x_already_normed=True)\n        with autocast():\n            pred=model(x)\n            loss=loss_fc(pred,y)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        optimizer.zero_grad()\n        scheduler.step()\n        score=dice_coef(pred.detach(),y)\n        losss=(losss*i+loss.item())/(i+1)\n        scores=(scores*i+score)/(i+1)\n        time.set_description(f\"epoch:{epoch},loss:{losss:.4f},score:{scores:.4f},lr{optimizer.param_groups[0]['lr']:.4e}\")\n        time.update()\n        del loss,pred\n    time.close()\n    \n    model.eval()\n    time=tqdm(range(len(val_dataset)))\n    val_losss=0\n    val_scores=0\n    val_hit=0\n    val_fp=0\n    for i,(x,y) in enumerate(val_dataset):\n        x=x.cuda().to(tc.float32)\n        y=y.cuda().to(tc.float32)\n        x=norm_with_clip(x.reshape(-1,*x.shape[2:])).reshape(x.shape)\n\n        with autocast():\n            with tc.no_grad():\n                pred=model(x)\n                loss=loss_fc(pred,y)\n        pred = pred.detach()\n        score=dice_coef(pred,y)\n        val_losss=(val_losss*i+loss.item())/(i+1)\n        val_scores=(val_scores*i+score)/(i+1)\n        hit, fp = np_metric(pred.numpy(force=True),y.numpy(force=True))\n        val_hit=(val_hit*i+hit)/(i+1)\n        val_fp=(val_fp*i+fp)/(i+1)\n        time.set_description(f\"val-->loss:{val_losss:.4f},score:{val_scores:.4f}, hit:{val_hit:.4f}, fp:{val_fp:.4f}\")\n        time.update()\n    \n#     if val_scores > best_score + margin:\n#         best_score = val_scores\n    if val_scores > 0.8:\n        tc.save(model.module.state_dict(),f\"./unet_{CFG.backbone}_{epoch}_loss{losss:.2f}_score{scores:.2f}_val_loss{val_losss:.2f}_val_score{val_scores:.2f}_val_hit{val_hit:.2f}_val_fp{val_fp:.2f}.pt\")\n\n\n    time.close()\ntc.save(model.module.state_dict(),f\"./unet_{CFG.backbone}_{epoch}_loss{losss:.2f}_score{scores:.2f}_val_loss{val_losss:.2f}_val_score{val_scores:.2f}_midd_1024_final.pt\")\n\ntime.close()","metadata":{"execution":{"iopub.status.busy":"2024-02-02T21:16:36.993106Z","iopub.execute_input":"2024-02-02T21:16:36.993497Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"model_name Unet\nbackbone tu-maxxvitv2_nano_rw_256\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/94.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45a72f391a9b4909a9103bb64ac970b0"}},"metadata":{}},{"name":"stderr","text":"epoch:0,loss:0.9778,score:0.0264,lr3.7713e-06:  10%|█         | 65/637 [01:22<12:06,  1.27s/it]\nepoch:0,loss:0.9475,score:0.1377,lr1.9044e-05: 100%|██████████| 637/637 [09:49<00:00,  1.08it/s]\n  0%|          | 0/138 [00:00<?, ?it/s]/tmp/ipykernel_47/3931055784.py:8: RuntimeWarning: invalid value encountered in scalar divide\n  return hit/t_sum, fp/p_sum\nval-->loss:0.9293,score:0.5114, hit:nan, fp:1.0000: 100%|██████████| 138/138 [02:30<00:00,  1.09s/it]   \nepoch:1,loss:0.7751,score:0.5005,lr5.4334e-05: 100%|██████████| 637/637 [09:11<00:00,  1.15it/s]\nval-->loss:0.6134,score:0.8369, hit:0.7864, fp:0.0211:  59%|█████▊    | 81/138 [01:18<00:54,  1.04it/s]","output_type":"stream"}]},{"cell_type":"code","source":"import gc\ngc.collect()\ntc.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-02-02T21:16:21.933884Z","iopub.execute_input":"2024-02-02T21:16:21.934263Z","iopub.status.idle":"2024-02-02T21:16:22.682761Z","shell.execute_reply.started":"2024-02-02T21:16:21.934232Z","shell.execute_reply":"2024-02-02T21:16:22.681782Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}