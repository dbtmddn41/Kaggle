{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":8084888,"sourceType":"datasetVersion","datasetId":4750478},{"sourceId":171590944,"sourceType":"kernelVersion"},{"sourceId":5112,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3900,"modelId":1902},{"sourceId":26154,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":22015,"modelId":3301}],"dockerImageVersionId":30675,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dbtmddn41/prompt-recovery-hf-tpu-train2?scriptVersionId=173092327\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"class CFG:\n    seed = 42\n    dataset_path = \"/kaggle/input/llm-prompt-recovery\"\n    preset = \"/kaggle/input/gemma/transformers/1.1-7b-it/1\"  # name of pretrained Gemma\n#     preset = \"/kaggle/input/llama-2/pytorch/7b-chat-hf/1\"\n    sequence_length = 1024 # max size of input sequence for training\n    train_batch = 8 # size of the input batch in training\n    validation_batch = 16\n    epochs = 3 # number of epochs to train\n    test_size = 0.\n    train_datas = ['kishanvavdara', 'newtonbaba12345_3', 'newtonbaba12345_1', 'aatiffraz', 'newtonbaba12345_2', 'host', 'huosiyuan_2']\n    validation_datas = [\"winddude\", 'juanmerinobermejo']\n    lora_rank=4","metadata":{"execution":{"iopub.status.busy":"2024-04-21T01:28:43.829896Z","iopub.execute_input":"2024-04-21T01:28:43.830127Z","iopub.status.idle":"2024-04-21T01:28:43.843251Z","shell.execute_reply.started":"2024-04-21T01:28:43.830101Z","shell.execute_reply":"2024-04-21T01:28:43.842592Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip3 install transformers datasets peft -Uq\n# !pip install torch~=2.1.0 --index-url https://download.pytorch.org/whl/cpu -q # Updating torch since we need the latest version\n# !pip install torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html -q\n!pip uninstall tensorflow -y # If we don't do this, TF will take over TPU and cause permission error for PT\n!cp /kaggle/input/d/dbtmddn41/utils-xla/spmd_util.py . # From this repo: https://github.com/HeegyuKim/torch-xla-SPMD","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-21T01:28:43.844388Z","iopub.execute_input":"2024-04-21T01:28:43.844605Z","iopub.status.idle":"2024-04-21T01:29:24.26661Z","shell.execute_reply.started":"2024-04-21T01:28:43.844582Z","shell.execute_reply":"2024-04-21T01:29:24.265505Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nFound existing installation: tensorflow 2.15.0\nUninstalling tensorflow-2.15.0:\n  Successfully uninstalled tensorflow-2.15.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport datasets\nimport torch.optim as optim\nimport torch_xla.debug.profiler as xp\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp # We also import mp modules if we wanna use that for some reason\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.test.test_utils as test_utils\nimport torch\nimport torch.nn as nn\nimport re\nimport torch_xla.experimental.xla_sharding as xs\nimport torch_xla.core.xla_model as xm\nfrom transformers import (\n    AutoTokenizer, AutoModelForCausalLM, DataCollatorWithPadding, AutoConfig\n) # You can use any of models with those configs (even flan T5 xxl!). Other models are not supported.\n\nfrom transformers import logging as hf_logging\nimport torch.nn.functional as F\nimport torch_xla.runtime as xr\n\nxr.use_spmd()\n\nimport torch_xla.experimental.xla_sharding as xs # \"experimental\" prefix always means you're gonna have a good time LMAO\nfrom torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor\nfrom torch_xla.experimental.xla_sharding import Mesh\n\nfrom peft import LoraConfig, TaskType, get_peft_model # If we wanna use peft. Quantazation requiers GPU though. You'll have to download already quantazed models\nfrom spmd_util import partition_module                # You could experiment with using already quantazed models like 4bit/Llama-2-7b-Chat-GPTQ if you're feeling funny\nfrom datasets import Dataset, load_dataset, concatenate_datasets\nfrom dataclasses import dataclass\nfrom tqdm import tqdm\n\nfrom peft import LoftQConfig, LoraConfig, get_peft_model\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig\nimport datasets\nimport pandas as pd\nimport numpy as np\nfrom datasets import Dataset\nfrom sklearn.metrics import roc_auc_score\n\n!export USE_TORCH=True #If we don't do this, transformers will seemingly bork the session upon import. Really weird error.\nos.environ[\"PJRT_DEVICE\"] = \"TPU\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ.pop('TPU_PROCESS_ADDRESSES')\nos.environ.pop('CLOUD_TPU_TASK_ID')\nhf_logging.set_verbosity_error() # It can still display warnings which is a bit annoying but whatever\n\n\n# MAX_INPUT=512\n# MODEL = \"/kaggle/input/gemma/transformers/7b-it/2\" #You should be able to use 13B model with no changes! There should be enough HBM\ndevice = xm.xla_device()","metadata":{"execution":{"iopub.status.busy":"2024-04-21T01:29:24.267929Z","iopub.execute_input":"2024-04-21T01:29:24.26823Z","iopub.status.idle":"2024-04-21T01:30:00.157706Z","shell.execute_reply.started":"2024-04-21T01:29:24.268198Z","shell.execute_reply":"2024-04-21T01:30:00.156279Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n/tmp/ipykernel_13/752609908.py:14: DeprecationWarning: Importing from `torch_xla.experimental.xla_sharding` will be deprecated after 2.2 release. Please use `torch_xla.distributed.spmd` instead.\n  import torch_xla.experimental.xla_sharding as xs\n/tmp/ipykernel_13/752609908.py:27: DeprecationWarning: Importing from `torch_xla.experimental.xla_sharded_tensor` will be deprecated after 2.2 release. Please use `torch_xla.distributed.spmd` instead.\n  from torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"\ntokenizer = AutoTokenizer.from_pretrained(CFG.preset)\ntokenizer.padding_side = 'right'\nmodel = AutoModelForCausalLM.from_pretrained(CFG.preset, torch_dtype=torch.bfloat16)\n\nif 'mistral' in CFG.preset:\n    tokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-04-21T01:30:00.160352Z","iopub.execute_input":"2024-04-21T01:30:00.161091Z","iopub.status.idle":"2024-04-21T01:30:26.727716Z","shell.execute_reply.started":"2024-04-21T01:30:00.161058Z","shell.execute_reply":"2024-04-21T01:30:26.726975Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Loading checkpoint shards: 100%|██████████| 4/4 [00:25<00:00,  6.35s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=CFG.lora_rank,\n    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    task_type=\"CAUSAL_LM\",\n    lora_dropout=0.05,\n    bias=\"none\"\n#     inference_mode=True\n)\npeft_model = get_peft_model(model, lora_config)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T01:30:26.728672Z","iopub.execute_input":"2024-04-21T01:30:26.728943Z","iopub.status.idle":"2024-04-21T01:30:27.235942Z","shell.execute_reply.started":"2024-04-21T01:30:26.728916Z","shell.execute_reply":"2024-04-21T01:30:27.235096Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"config = AutoConfig.from_pretrained(CFG.preset)\nnum_devices = xr.global_runtime_device_count()\nprint(num_devices)\nmesh_shape = (1, num_devices, 1, 1)\ndevice_ids = np.array(range(num_devices))\nmesh = Mesh(device_ids, mesh_shape, ('dp', 'fsdp', 'mp', 'sp'))\npartition_module(peft_model, mesh) # After this, the model is sharded between cores but still has the same API as if it was on single device. Neat.\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T01:30:27.237143Z","iopub.execute_input":"2024-04-21T01:30:27.237536Z","iopub.status.idle":"2024-04-21T01:30:41.240871Z","shell.execute_reply.started":"2024-04-21T01:30:27.237503Z","shell.execute_reply":"2024-04-21T01:30:41.239989Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"E0421 01:30:29.177498336      13 oauth2_credentials.cc:238]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2024-04-21T01:30:29.17748233+00:00\", grpc_status:2}\n","output_type":"stream"},{"name":"stdout","text":"8\n","output_type":"stream"}]},{"cell_type":"code","source":"# inputs = tokenizer(\"hello from the other side\", return_tensors='pt')\n# input_ids, attention_mask = inputs.input_ids.to(device), inputs.attention_mask.to(device)\n# xs.mark_sharding(input_ids, mesh, (0, 1))\n# xs.mark_sharding(attention_mask, mesh, (0, 1))\n# output = peft_model(input_ids, attention_mask=attention_mask, labels=input_ids)\n# output.loss","metadata":{"execution":{"iopub.status.busy":"2024-04-21T01:30:41.241896Z","iopub.execute_input":"2024-04-21T01:30:41.24216Z","iopub.status.idle":"2024-04-21T01:30:41.245573Z","shell.execute_reply.started":"2024-04-21T01:30:41.242133Z","shell.execute_reply":"2024-04-21T01:30:41.244867Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# import time\n# for text in [\"<s> [INST] Original Text:{}\\nRewritten Text:{}\\nWrite a prompt that was likely given to the LLM to rewrite original text into rewritten text. [/INST]\".format(val_df.iloc[0][\"original_text\"], val_df.iloc[0][\"rewritten_text\"])]:\n#     start_time = time.time()\n#     inputs = tokenizer(text, return_tensors='pt')\n#     input_ids, attention_mask = inputs.input_ids.to(device), inputs.attention_mask.to(device)\n#     xs.mark_sharding(input_ids, mesh, (0, 1))\n#     xs.mark_sharding(attention_mask, mesh, (0, 1))\n#     output = peft_model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=500)\n#     print(tokenizer.batch_decode(output))\n#     print(\"걸린 시간: \", time.time() - start_time)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T01:30:41.246398Z","iopub.execute_input":"2024-04-21T01:30:41.246662Z","iopub.status.idle":"2024-04-21T01:30:41.262117Z","shell.execute_reply.started":"2024-04-21T01:30:41.246618Z","shell.execute_reply":"2024-04-21T01:30:41.261454Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"peft_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-04-21T01:30:41.264068Z","iopub.execute_input":"2024-04-21T01:30:41.264321Z","iopub.status.idle":"2024-04-21T01:30:41.278862Z","shell.execute_reply.started":"2024-04-21T01:30:41.264296Z","shell.execute_reply":"2024-04-21T01:30:41.278104Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"trainable params: 12,500,992 || all params: 9,336,613,888 || trainable%: 0.13389213852001589\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# datasets","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport gc","metadata":{"execution":{"iopub.status.busy":"2024-04-21T01:30:41.27968Z","iopub.execute_input":"2024-04-21T01:30:41.279942Z","iopub.status.idle":"2024-04-21T01:30:41.288763Z","shell.execute_reply.started":"2024-04-21T01:30:41.279915Z","shell.execute_reply":"2024-04-21T01:30:41.288064Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"%%time\ndf = pd.read_csv(\"/kaggle/input/all-in-one-dataset-with-embedding/df_with_emb_20240412.csv\")\n\nfloat_cols = df.select_dtypes('float64').columns\ndf[float_cols] = df[float_cols].astype('float32')\ndf = df[df['rewritten_text'].str.len() >= 20]\ntrain_df = df[df['dataset_id'].isin(CFG.train_datas)].copy(deep=True)\nval_df = df[df['dataset_id'].isin(CFG.validation_datas)].sample(n=1300).copy(deep=True)\n\n# df = df.drop(index=0)\ngc.collect()\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-21T01:30:41.289505Z","iopub.execute_input":"2024-04-21T01:30:41.289718Z","iopub.status.idle":"2024-04-21T01:32:56.442516Z","shell.execute_reply.started":"2024-04-21T01:30:41.289695Z","shell.execute_reply":"2024-04-21T01:32:56.441665Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"CPU times: user 1min 19s, sys: 4.64 s, total: 1min 24s\nWall time: 2min 15s\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                                           original_text  \\\n0      Dear Randy,\\n\\nI hope this letter finds you we...   \n14656  It'd been three days since Baldr Alwisson had ...   \n14657  I am the daemon that all men do seek, \\n \\n Du...   \n14658  Come here children, listen to me explain \\n Th...   \n14659  *I... Am? * \\n \\n You are. \\n \\n *I am. * \\n \\...   \n\n                                          rewrite_prompt  \\\n0      Rephrase this letter to infuse it with an elfi...   \n14656  Rewrite this text in the style of a Victorian ...   \n14657  Translate the essence of this text into a ghos...   \n14658  Rewrite this text in the style of a ancient Ro...   \n14659  Translate the essence of this text into a the ...   \n\n                                          rewritten_text         dataset_id  \\\n0      Dear Randy,\\n\\nMay this enchanted message find...               host   \n14656  \\n\\nHere is the text rewritten in the style of...  newtonbaba12345_1   \n14657  \\n\\nSure, here's the ghost narrative:\\n\\nIn th...  newtonbaba12345_1   \n14658  \\n\\nSure, here is the text rewritten in the st...  newtonbaba12345_1   \n14659  \\n\\n**Roaring Twenties Narrative:**\\n\\nThe Roa...  newtonbaba12345_1   \n\n       original_text_emb_0  original_text_emb_1  original_text_emb_2  \\\n0                -0.028678            -0.072260            -0.003482   \n14656            -0.030413            -0.067652             0.049466   \n14657            -0.014884            -0.078367             0.023980   \n14658            -0.019390            -0.077755             0.044897   \n14659            -0.009235            -0.055168             0.022361   \n\n       original_text_emb_3  original_text_emb_4  original_text_emb_5  ...  \\\n0                 0.050595            -0.010600            -0.023786  ...   \n14656             0.035858             0.008359            -0.015908  ...   \n14657             0.069455            -0.034322            -0.023850  ...   \n14658             0.056840            -0.016640            -0.033011  ...   \n14659             0.036347            -0.000441            -0.013944  ...   \n\n       rewritten_text_emb_758  rewritten_text_emb_759  rewritten_text_emb_760  \\\n0                   -0.023307                0.011079               -0.026521   \n14656               -0.033934               -0.027861               -0.033822   \n14657               -0.001434               -0.021219               -0.001819   \n14658               -0.045966                0.003527               -0.030286   \n14659               -0.033253               -0.031921               -0.006217   \n\n       rewritten_text_emb_761  rewritten_text_emb_762  rewritten_text_emb_763  \\\n0                   -0.035838               -0.011306                0.035690   \n14656               -0.052785               -0.000612                0.051786   \n14657               -0.044634                0.004461                0.038283   \n14658               -0.054125                0.024585                0.031940   \n14659               -0.032138                0.001856                0.040625   \n\n       rewritten_text_emb_764  rewritten_text_emb_765  rewritten_text_emb_766  \\\n0                   -0.005810                0.004506               -0.037956   \n14656                0.012271                0.003726               -0.018933   \n14657                0.017500                0.011950               -0.003624   \n14658                0.022491               -0.000578               -0.025836   \n14659                0.024044                0.026046               -0.044090   \n\n       rewritten_text_emb_767  \n0                    0.001139  \n14656               -0.026539  \n14657               -0.032714  \n14658               -0.027405  \n14659               -0.009349  \n\n[5 rows x 2308 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>original_text</th>\n      <th>rewrite_prompt</th>\n      <th>rewritten_text</th>\n      <th>dataset_id</th>\n      <th>original_text_emb_0</th>\n      <th>original_text_emb_1</th>\n      <th>original_text_emb_2</th>\n      <th>original_text_emb_3</th>\n      <th>original_text_emb_4</th>\n      <th>original_text_emb_5</th>\n      <th>...</th>\n      <th>rewritten_text_emb_758</th>\n      <th>rewritten_text_emb_759</th>\n      <th>rewritten_text_emb_760</th>\n      <th>rewritten_text_emb_761</th>\n      <th>rewritten_text_emb_762</th>\n      <th>rewritten_text_emb_763</th>\n      <th>rewritten_text_emb_764</th>\n      <th>rewritten_text_emb_765</th>\n      <th>rewritten_text_emb_766</th>\n      <th>rewritten_text_emb_767</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Dear Randy,\\n\\nI hope this letter finds you we...</td>\n      <td>Rephrase this letter to infuse it with an elfi...</td>\n      <td>Dear Randy,\\n\\nMay this enchanted message find...</td>\n      <td>host</td>\n      <td>-0.028678</td>\n      <td>-0.072260</td>\n      <td>-0.003482</td>\n      <td>0.050595</td>\n      <td>-0.010600</td>\n      <td>-0.023786</td>\n      <td>...</td>\n      <td>-0.023307</td>\n      <td>0.011079</td>\n      <td>-0.026521</td>\n      <td>-0.035838</td>\n      <td>-0.011306</td>\n      <td>0.035690</td>\n      <td>-0.005810</td>\n      <td>0.004506</td>\n      <td>-0.037956</td>\n      <td>0.001139</td>\n    </tr>\n    <tr>\n      <th>14656</th>\n      <td>It'd been three days since Baldr Alwisson had ...</td>\n      <td>Rewrite this text in the style of a Victorian ...</td>\n      <td>\\n\\nHere is the text rewritten in the style of...</td>\n      <td>newtonbaba12345_1</td>\n      <td>-0.030413</td>\n      <td>-0.067652</td>\n      <td>0.049466</td>\n      <td>0.035858</td>\n      <td>0.008359</td>\n      <td>-0.015908</td>\n      <td>...</td>\n      <td>-0.033934</td>\n      <td>-0.027861</td>\n      <td>-0.033822</td>\n      <td>-0.052785</td>\n      <td>-0.000612</td>\n      <td>0.051786</td>\n      <td>0.012271</td>\n      <td>0.003726</td>\n      <td>-0.018933</td>\n      <td>-0.026539</td>\n    </tr>\n    <tr>\n      <th>14657</th>\n      <td>I am the daemon that all men do seek, \\n \\n Du...</td>\n      <td>Translate the essence of this text into a ghos...</td>\n      <td>\\n\\nSure, here's the ghost narrative:\\n\\nIn th...</td>\n      <td>newtonbaba12345_1</td>\n      <td>-0.014884</td>\n      <td>-0.078367</td>\n      <td>0.023980</td>\n      <td>0.069455</td>\n      <td>-0.034322</td>\n      <td>-0.023850</td>\n      <td>...</td>\n      <td>-0.001434</td>\n      <td>-0.021219</td>\n      <td>-0.001819</td>\n      <td>-0.044634</td>\n      <td>0.004461</td>\n      <td>0.038283</td>\n      <td>0.017500</td>\n      <td>0.011950</td>\n      <td>-0.003624</td>\n      <td>-0.032714</td>\n    </tr>\n    <tr>\n      <th>14658</th>\n      <td>Come here children, listen to me explain \\n Th...</td>\n      <td>Rewrite this text in the style of a ancient Ro...</td>\n      <td>\\n\\nSure, here is the text rewritten in the st...</td>\n      <td>newtonbaba12345_1</td>\n      <td>-0.019390</td>\n      <td>-0.077755</td>\n      <td>0.044897</td>\n      <td>0.056840</td>\n      <td>-0.016640</td>\n      <td>-0.033011</td>\n      <td>...</td>\n      <td>-0.045966</td>\n      <td>0.003527</td>\n      <td>-0.030286</td>\n      <td>-0.054125</td>\n      <td>0.024585</td>\n      <td>0.031940</td>\n      <td>0.022491</td>\n      <td>-0.000578</td>\n      <td>-0.025836</td>\n      <td>-0.027405</td>\n    </tr>\n    <tr>\n      <th>14659</th>\n      <td>*I... Am? * \\n \\n You are. \\n \\n *I am. * \\n \\...</td>\n      <td>Translate the essence of this text into a the ...</td>\n      <td>\\n\\n**Roaring Twenties Narrative:**\\n\\nThe Roa...</td>\n      <td>newtonbaba12345_1</td>\n      <td>-0.009235</td>\n      <td>-0.055168</td>\n      <td>0.022361</td>\n      <td>0.036347</td>\n      <td>-0.000441</td>\n      <td>-0.013944</td>\n      <td>...</td>\n      <td>-0.033253</td>\n      <td>-0.031921</td>\n      <td>-0.006217</td>\n      <td>-0.032138</td>\n      <td>0.001856</td>\n      <td>0.040625</td>\n      <td>0.024044</td>\n      <td>0.026046</td>\n      <td>-0.044090</td>\n      <td>-0.009349</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 2308 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def truncate_txt(text, length):\n    text_list = text.split()\n    \n    if len(text_list) <= length:\n        return text\n    \n    return \" \".join(text_list[:length])\n\n\ndef gen_val_prompt(df):\n    \n    # Truncate the texts to first 200 words for now\n    # As we are having memory issues on Mixtral8x7b\n    og_text = truncate_txt(df[\"original_text\"].strip(), CFG.sequence_length//3)\n    rewritten_text = truncate_txt(df[\"rewritten_text\"].strip(), CFG.sequence_length//3)\n    template = \"\"\"<start_of_turn>user\\nOriginal Text:{}\\nRewritten Text:{}\\nWrite a prompt that was likely given to the LLM to rewrite original text into rewritten text.<end_of_turn>\n<start_of_turn>model\nOutput:\n\"\"\"\n    return template.format(og_text, rewritten_text).strip()\n\ndef gen_prompt(df):\n    \n    # Truncate the texts to first 200 words for now\n    # As we are having memory issues on Mixtral8x7b\n    og_text = truncate_txt(df[\"original_text\"].strip(), CFG.sequence_length//3)\n    rewritten_text = truncate_txt(df[\"rewritten_text\"].strip(), CFG.sequence_length//3)\n    rewrite_prompt = truncate_txt(df[\"rewrite_prompt\"].strip(), CFG.sequence_length//3)\n    template = \"\"\"<start_of_turn>user\\nOriginal Text:{}\\nRewritten Text:{}\\nWrite a prompt that was likely given to the LLM to rewrite original text into rewritten text.<end_of_turn>\n<start_of_turn>model\nOutput: {}<end_of_turn><eos>\"\"\"\n    return template.format(og_text, rewritten_text, rewrite_prompt).strip()","metadata":{"execution":{"iopub.status.busy":"2024-04-21T01:32:56.443701Z","iopub.execute_input":"2024-04-21T01:32:56.444001Z","iopub.status.idle":"2024-04-21T01:32:56.451613Z","shell.execute_reply.started":"2024-04-21T01:32:56.443972Z","shell.execute_reply":"2024-04-21T01:32:56.450718Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom datasets import Dataset, DatasetDict\n\ntqdm.pandas()\n# ds = DatasetDict()\ndef preprocess_func(example):\n    inputs = tokenizer(example['prompt'], truncation=True, max_length=CFG.sequence_length, padding='max_length')\n    return (\n    {\n        'input_ids': inputs.input_ids,\n        'attention_mask': inputs.attention_mask\n    })\n\ntrain_df['prompt'] = train_df[[\"original_text\", \"rewritten_text\", \"rewrite_prompt\"]].progress_apply(gen_prompt, axis=1)\ntrain_ds = Dataset.from_pandas(train_df[['prompt']])\ntrain_ds = train_ds.map(preprocess_func)\ntrain_ds = train_ds.remove_columns([\"__index_level_0__\", \"prompt\"])\n\nval_df['prompt'] = val_df[[\"original_text\", \"rewritten_text\", \"rewrite_prompt\"]].progress_apply(gen_prompt, axis=1)\nval_ds = Dataset.from_pandas(val_df[['prompt']])\nval_ds = val_ds.map(preprocess_func)\nval_ds = val_ds.remove_columns([\"__index_level_0__\", \"prompt\"])\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T01:32:56.452687Z","iopub.execute_input":"2024-04-21T01:32:56.452938Z","iopub.status.idle":"2024-04-21T01:33:10.849009Z","shell.execute_reply.started":"2024-04-21T01:32:56.452912Z","shell.execute_reply":"2024-04-21T01:33:10.848207Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"100%|██████████| 5617/5617 [00:00<00:00, 30731.93it/s]\nMap: 100%|██████████| 5617/5617 [00:09<00:00, 565.29 examples/s]\n100%|██████████| 1300/1300 [00:00<00:00, 19076.56it/s]\nMap: 100%|██████████| 1300/1300 [00:03<00:00, 416.01 examples/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\ntraindata_loader = torch.utils.data.DataLoader(train_ds, batch_size=CFG.train_batch, collate_fn=DataCollatorWithPadding(tokenizer=tokenizer),shuffle=True, num_workers=8)\nvaldata_loader = torch.utils.data.DataLoader(val_ds, batch_size=CFG.validation_batch, collate_fn=DataCollatorWithPadding(tokenizer=tokenizer), num_workers=8)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T01:33:10.850005Z","iopub.execute_input":"2024-04-21T01:33:10.850303Z","iopub.status.idle":"2024-04-21T01:33:10.855462Z","shell.execute_reply.started":"2024-04-21T01:33:10.850273Z","shell.execute_reply":"2024-04-21T01:33:10.85468Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Train!","metadata":{}},{"cell_type":"code","source":"model.eval()\ntotal_val_loss = 0.0\npbar = tqdm(range(len(valdata_loader)))#, disable=True)\ndata_iter = iter(valdata_loader)\nlogging_steps=1\nwith torch.no_grad():\n    for step, batch in enumerate(pbar):\n        batch = next(data_iter)\n        input_ids, attention_mask = batch.input_ids.to(device), batch.attention_mask.to(device)\n        xs.mark_sharding(input_ids, mesh, (0, 1))\n        xs.mark_sharding(attention_mask, mesh, (0, 1))\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n        loss = outputs.loss\n        total_val_loss = (total_val_loss * step + loss.item()) / (step + 1)\n        if (step + 1) % logging_steps == 0:\n            pbar.set_postfix({'val_loss': total_val_loss, 'step': step+1})\nprint(total_val_loss)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T01:33:10.856345Z","iopub.execute_input":"2024-04-21T01:33:10.856572Z","iopub.status.idle":"2024-04-21T01:36:07.526322Z","shell.execute_reply.started":"2024-04-21T01:33:10.856548Z","shell.execute_reply":"2024-04-21T01:36:07.52497Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"100%|██████████| 82/82 [02:56<00:00,  2.15s/it, val_loss=24.3, step=82]","output_type":"stream"},{"name":"stdout","text":"24.263721256721315\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"!export XLA_USE_BF16=1\ndef train(\n    model, train_data, validation_data=None, train_batch=4, validation_batch=8, epochs=10, logging_steps=1,\n    lr=1e-5, \n):\n#     def train_model(model, input_ids, attention_mask, optimizer):\n#         output = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n#         loss = output.loss\n#         loss.backward()\n#         optimizer.step()\n#         return loss\n    \n#     compiled_step = torch.compile(model, backend=\"openxla\")\n    \n    optimizer = optim.AdamW(model.parameters(), lr=lr)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=6000/train_batch*epochs)\n    for epoch in range(1, epochs + 1):\n        model.train()\n        xm.master_print('Epoch {} train begin {}'.format(epoch, test_utils.now()))\n        pbar = tqdm(range(len(train_data)))#, disable=True)\n        data_iter = iter(train_data)\n        total_loss = 0.\n#         with torch.autocast(\"xla\", dtype=torch.bfloat16):\n        for step, batch in enumerate(pbar):\n            batch = next(data_iter)\n            optimizer.zero_grad()\n            input_ids, attention_mask = batch.input_ids.to(device), batch.attention_mask.to(device)\n            xs.mark_sharding(input_ids, mesh, (0, 1)) # Sharding inputs\n            xs.mark_sharding(attention_mask, mesh, (0, 1))\n\n#                 loss = compiled_step(model, input_ids, attention_mask, optimizer)\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n            xm.mark_step()\n            total_loss = (total_loss * step + loss.item()) / (step + 1)\n            if (step + 1) % logging_steps == 0:\n                pbar.set_postfix({'loss': total_loss, 'step': step+1, 'epoch': epoch})\n            scheduler.step()\n#             if step > 10:\n#                 break\n        xm.master_print('Epoch {} train end {}, loss={:.3f}'.format(epoch, test_utils.now(), total_loss))\n        model.eval()\n        total_val_loss = 0.0\n        if validation_data is not None:\n            pbar = tqdm(range(len(validation_data)))#, disable=True)\n            data_iter = iter(validation_data)\n            with torch.no_grad():\n                for step, batch in enumerate(pbar):\n                    batch = next(data_iter)\n                    input_ids, attention_mask = batch.input_ids.to(device), batch.attention_mask.to(device)\n                    xs.mark_sharding(input_ids, mesh, (0, 1))\n                    xs.mark_sharding(attention_mask, mesh, (0, 1))\n                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n                    loss = outputs.loss\n                    total_val_loss = (total_val_loss * step + loss.item()) / (step + 1)\n                    if (step + 1) % logging_steps == 0:\n                        pbar.set_postfix({'val_loss': total_val_loss, 'step': step+1, 'epoch': epoch})\n\n            xm.master_print('Epoch {} test end {}, test val_loss={:.3f}'.format(epoch, test_utils.now(), total_val_loss))\n        \n\n        \n    model.cpu()\n    model.save_pretrained(f\"{epoch}-lora_gemmal7b-1.1\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T01:36:07.530018Z","iopub.execute_input":"2024-04-21T01:36:07.530345Z","iopub.status.idle":"2024-04-21T01:36:08.491078Z","shell.execute_reply.started":"2024-04-21T01:36:07.530301Z","shell.execute_reply":"2024-04-21T01:36:08.489784Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"train(peft_model, traindata_loader, validation_data=valdata_loader, train_batch=CFG.train_batch, epochs=CFG.epochs, lr=8e-5)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T01:36:08.492561Z","iopub.execute_input":"2024-04-21T01:36:08.492865Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1 train begin 01:36:08\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/703 [00:00<?, ?it/s]/usr/local/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: aten::reshape: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n  6%|▌         | 39/703 [06:17<37:21,  3.38s/it, loss=11.6, step=39, epoch=1]  ","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
