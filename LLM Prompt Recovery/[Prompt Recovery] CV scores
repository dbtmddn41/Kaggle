{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7729275,"sourceType":"datasetVersion","datasetId":4516250},{"sourceId":7930545,"sourceType":"datasetVersion","datasetId":4661387},{"sourceId":168291408,"sourceType":"kernelVersion"},{"sourceId":170304575,"sourceType":"kernelVersion"},{"sourceId":171590944,"sourceType":"kernelVersion"}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2024-04-15T16:08:42.285344Z","iopub.execute_input":"2024-04-15T16:08:42.285631Z","iopub.status.idle":"2024-04-15T16:08:55.613642Z","shell.execute_reply.started":"2024-04-15T16:08:42.285604Z","shell.execute_reply":"2024-04-15T16:08:55.612738Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting sentence-transformers\n  Downloading sentence_transformers-2.6.1-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.38.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.1)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.26.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.4)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.21.4)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.3.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (21.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.15.1->sentence-transformers) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-2.6.1-py3-none-any.whl (163 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.3/163.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-2.6.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport glob\ntqdm.pandas()\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity","metadata":{"execution":{"iopub.status.busy":"2024-04-15T16:08:55.615431Z","iopub.execute_input":"2024-04-15T16:08:55.615716Z","iopub.status.idle":"2024-04-15T16:09:03.562597Z","shell.execute_reply.started":"2024-04-15T16:08:55.615691Z","shell.execute_reply":"2024-04-15T16:09:03.561788Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"model = SentenceTransformer('/kaggle/input/sentence-t5-base-hf/sentence-t5-base')","metadata":{"execution":{"iopub.status.busy":"2024-04-15T16:09:03.564129Z","iopub.execute_input":"2024-04-15T16:09:03.564684Z","iopub.status.idle":"2024-04-15T16:09:07.813197Z","shell.execute_reply.started":"2024-04-15T16:09:03.564630Z","shell.execute_reply":"2024-04-15T16:09:07.812316Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"여기부터 cv","metadata":{}},{"cell_type":"code","source":"# validation_df = pd.read_csv(\"/kaggle/input/prompt-recovery-gemma-keras-tpu-train/validation.csv\")\n# ans_embedded = validation_df.filter(like=\"rewrite_prompt_emb\").to_numpy()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T16:09:07.815356Z","iopub.execute_input":"2024-04-15T16:09:07.815833Z","iopub.status.idle":"2024-04-15T16:09:07.819947Z","shell.execute_reply.started":"2024-04-15T16:09:07.815806Z","shell.execute_reply":"2024-04-15T16:09:07.818973Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# val_path = \"/kaggle/input/prompt-recovery-gemma-keras-tpu-train/\"\n# predicted_df_paths = glob.glob(val_path+\"*_validation.csv\")\n# scores = {}\n# for df_path in predicted_df_paths:\n#     step = ' '.join(df_path.split('/')[-1].split('_')[:2])\n#     df = pd.read_csv(df_path)\n# #     assert all(validation_df.iloc[:, 0] == df.iloc[:, 0])\n#     preds = df['rewritte_prompt_predicted'].apply(lambda x: x[x.find('<start_of_turn>model'):].replace('<start_of_turn>model\\n', '').replace('<end_of_turn><eos>','').strip())\n#     preds_embedded = model.encode(preds, normalize_embeddings=True, show_progress_bar=True)\n#     score = 0.\n#     for i in range(len(preds_embedded)):\n#         score += cosine_similarity(ans_embedded[i].reshape(1,-1), preds_embedded[i].reshape(1,-1))**3\n#     score /= len(preds_embedded)\n#     scores[step] = float(score.squeeze())","metadata":{"execution":{"iopub.status.busy":"2024-04-15T16:09:07.820963Z","iopub.execute_input":"2024-04-15T16:09:07.821231Z","iopub.status.idle":"2024-04-15T16:09:07.831792Z","shell.execute_reply.started":"2024-04-15T16:09:07.821207Z","shell.execute_reply":"2024-04-15T16:09:07.830927Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# pd.DataFrame(scores, columns=['score']).to_csv('scores.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-15T16:09:07.832725Z","iopub.execute_input":"2024-04-15T16:09:07.832981Z","iopub.status.idle":"2024-04-15T16:09:07.840932Z","shell.execute_reply.started":"2024-04-15T16:09:07.832954Z","shell.execute_reply":"2024-04-15T16:09:07.840055Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# scores","metadata":{"execution":{"iopub.status.busy":"2024-04-05T00:17:27.184854Z","iopub.execute_input":"2024-04-05T00:17:27.185249Z","iopub.status.idle":"2024-04-05T00:17:27.196713Z","shell.execute_reply.started":"2024-04-05T00:17:27.185215Z","shell.execute_reply":"2024-04-05T00:17:27.195729Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'0 validation.csv': 0.5772503733135238,\n '2 validation.csv': 0.5803037134320542,\n '1 validation.csv': 0.5830339817102054,\n '3 validation.csv': 0.5807128794964261,\n 'final validation.csv': 0.5807128794964261}"},"metadata":{}}]},{"cell_type":"markdown","source":"여기부터 dataset select","metadata":{}},{"cell_type":"code","source":"validation_df = pd.read_csv(\"/kaggle/input/all-in-one-dataset-with-embedding/df_with_emb_20240412.csv\")\n# additional_df = pd.read_csv(\"/kaggle/input/llmpr-public-10k-unique/public_10k_unique_rewrite_prompt.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-04-15T16:10:01.698847Z","iopub.execute_input":"2024-04-15T16:10:01.699234Z","iopub.status.idle":"2024-04-15T16:12:21.062561Z","shell.execute_reply.started":"2024-04-15T16:10:01.699201Z","shell.execute_reply":"2024-04-15T16:12:21.061687Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"rewrite_prompts = [\n    \"Please improve this text using the writing style with maintaining the original meaning but altering the tone.\", \n    \"Please improve the following text by reimagining it through the lens of [insert desired style here], retaining the original essence while elevating its clarity, eloquence, and potency by modulating the tone, word choice, and stylistic nuances to harmoniously embody the stylistic features while ensuring the core message remains intact.\",\n    'Please improve the following text using the writing style of, maintaining the original meaning but altering the tone, diction, and stylistic elements to match the new style.Enhance the clarity, elegance, and impact of the following text by adopting the writing style of , ensuring the core message remains intact while transforming the tone, word choice, and stylistic features to align with the specified style.',\n    \"Improve the text to this.\",\n    \"Improve the following text.\",\n    \"Improve that text.\",\n    \"Rewrite the essay with a focus on a specific audience's interests and values. Tailor the language, examples, and even the structure to resonate with that particular group.\",\n    \"Rephrase the essay using a more analytical approach. Break down complex ideas, provide evidence, and analyze their significance in relation to the central theme.\",    \n]\nlb_scores = np.array([0.63, 0.62, 0.61, 0.60, 0.59, 0.58, 0.51, 0.49])\npreds_embedded = model.encode(rewrite_prompts, normalize_embeddings=True, show_progress_bar=True)\nd = {'dataset_id': [], 'avg_error': [], 'length': []}\nfor i in range(1, len(rewrite_prompts)+1):\n    d[f'scores{i}'] = []\n    d[f'errors{i}'] = []\nfor id, df in tqdm(validation_df.groupby(\"dataset_id\")):\n    ans_embedded = df.filter(like=\"rewrite_prompt_emb\")\n#     ans_embedded = model.encode(df[\"rewrite_prompt\"].tolist(), normalize_embeddings=True, show_progress_bar=True)#df.filter(like=\"rewrite_prompt_emb\")\n    d['dataset_id'].append(id)\n    score = (cosine_similarity(ans_embedded, preds_embedded)**3).mean(axis=0)\n    for i in range(len(rewrite_prompts)):\n        \n        d[f'scores{i+1}'].append(score[i])\n        d[f'errors{i+1}'].append(abs(score[i]-lb_scores[i]))\n    d['avg_error'].append(np.absolute(score-lb_scores).mean())\n    d['length'].append(len(df))\n    \nscores = pd.DataFrame(d)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T16:12:21.065431Z","iopub.execute_input":"2024-04-15T16:12:21.065729Z","iopub.status.idle":"2024-04-15T16:12:23.540582Z","shell.execute_reply.started":"2024-04-15T16:12:21.065705Z","shell.execute_reply":"2024-04-15T16:12:23.539295Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baa834670b2049c38063bacbc28df99c"}},"metadata":{}},{"name":"stderr","text":"100%|██████████| 28/28 [00:02<00:00, 11.75it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"scores = scores.sort_values('avg_error')\nscores","metadata":{"execution":{"iopub.status.busy":"2024-04-15T16:12:31.533885Z","iopub.execute_input":"2024-04-15T16:12:31.534515Z","iopub.status.idle":"2024-04-15T16:12:31.576178Z","shell.execute_reply.started":"2024-04-15T16:12:31.534483Z","shell.execute_reply":"2024-04-15T16:12:31.575375Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"           dataset_id  avg_error  length   scores1   errors1   scores2  \\\n18      kishanvavdara   0.020993     180  0.649414  0.019414  0.630006   \n27           winddude   0.024018   62852  0.595853  0.034147  0.575326   \n23  newtonbaba12345_3   0.026929    1795  0.592832  0.037168  0.576340   \n21  newtonbaba12345_1   0.032231    1000  0.587552  0.042448  0.552869   \n9                host   0.033217       1  0.587403  0.042597  0.598599   \n0           aatiffraz   0.033696    1000  0.586325  0.043675  0.552084   \n22  newtonbaba12345_2   0.034451    1541  0.584914  0.045086  0.550914   \n17  juanmerinobermejo   0.040164   14031  0.639861  0.009861  0.642972   \n11        huosiyuan_2   0.043354     100  0.620142  0.009858  0.685030   \n20           nbroad_2   0.067890    2400  0.548175  0.081825  0.502295   \n4         alexxxsem_2   0.068758    3991  0.548970  0.081030  0.520243   \n5         alexxxsem_3   0.069895    5371  0.547754  0.082246  0.519098   \n2    aishaalmahmoud_2   0.076295   15597  0.526221  0.103779  0.514402   \n26         thedrcat_3   0.079563    9140  0.514268  0.115732  0.499633   \n12        huosiyuan_3   0.082411      69  0.740298  0.110298  0.733707   \n8             gali1eo   0.084614   20507  0.508259  0.121741  0.491093   \n13        huosiyuan_4   0.088523      73  0.702262  0.072262  0.741135   \n10        huosiyuan_1   0.088642     376  0.716723  0.086723  0.763613   \n1    aishaalmahmoud_1   0.089077    4234  0.500417  0.129583  0.497672   \n3         alexxxsem_1   0.089292     224  0.516868  0.113132  0.498966   \n19           nbroad_1   0.090520    2166  0.522781  0.107219  0.510654   \n6            dipamc77   0.090866    2004  0.502789  0.127211  0.481107   \n14        huosiyuan_5   0.106236      44  0.702661  0.072661  0.761279   \n25         thedrcat_2   0.108396     860  0.482486  0.147514  0.472132   \n7      dschettler8845   0.110380    1000  0.480585  0.149415  0.471443   \n24         thedrcat_1   0.117457      89  0.482441  0.147559  0.457324   \n15    isakatsuyoshi_1   0.151681       5  0.450158  0.179842  0.425723   \n16    isakatsuyoshi_2   0.158763       3  0.465808  0.164192  0.438457   \n\n     errors2   scores3   errors3   scores4   errors4   scores5   errors5  \\\n18  0.010006  0.632212  0.022212  0.606695  0.006695  0.604950  0.014950   \n27  0.044674  0.560364  0.049636  0.610081  0.010081  0.601749  0.011749   \n23  0.043660  0.572451  0.037549  0.564640  0.035360  0.582835  0.007165   \n21  0.067131  0.542442  0.067558  0.595353  0.004647  0.582101  0.007899   \n9   0.021401  0.570934  0.039066  0.546613  0.053387  0.572783  0.017217   \n0   0.067916  0.541467  0.068533  0.593825  0.006175  0.581102  0.008898   \n22  0.069086  0.540632  0.069368  0.592042  0.007958  0.579052  0.010948   \n17  0.022972  0.633004  0.023004  0.610931  0.010931  0.633614  0.043614   \n11  0.065030  0.688003  0.078003  0.575344  0.024656  0.581030  0.008970   \n20  0.117705  0.501688  0.108312  0.509477  0.090523  0.534027  0.055973   \n4   0.099757  0.489936  0.120064  0.570146  0.029854  0.540213  0.049787   \n5   0.100902  0.488494  0.121506  0.569513  0.030487  0.539531  0.050469   \n2   0.105598  0.499269  0.110731  0.533285  0.066715  0.526269  0.063731   \n26  0.120367  0.485819  0.124181  0.558524  0.041476  0.528921  0.061079   \n12  0.113707  0.747077  0.137077  0.638633  0.038633  0.647296  0.057296   \n8   0.128907  0.478402  0.131598  0.551151  0.048849  0.523252  0.066748   \n13  0.121135  0.729188  0.119188  0.663045  0.063045  0.668933  0.078933   \n10  0.143613  0.779660  0.169660  0.638566  0.038566  0.645883  0.055883   \n1   0.122328  0.482927  0.127073  0.523195  0.076805  0.505694  0.084306   \n3   0.121034  0.469127  0.140873  0.551874  0.048126  0.510928  0.079072   \n19  0.109346  0.513224  0.096776  0.487186  0.112814  0.486453  0.103547   \n6   0.138893  0.469855  0.140145  0.541341  0.058659  0.515115  0.074885   \n14  0.141279  0.764627  0.154627  0.655224  0.055224  0.685028  0.095028   \n25  0.147868  0.451916  0.158084  0.530429  0.069571  0.493945  0.096055   \n7   0.148557  0.450902  0.159098  0.526768  0.073232  0.490650  0.099350   \n24  0.162676  0.438741  0.171259  0.531514  0.068486  0.492646  0.097354   \n15  0.194277  0.418465  0.191535  0.488909  0.111091  0.456336  0.133664   \n16  0.181543  0.403398  0.206602  0.466924  0.133076  0.444128  0.145872   \n\n     scores6   errors6   scores7   errors7   scores8   errors8  \n18  0.603744  0.023744  0.534911  0.024911  0.536016  0.046016  \n27  0.587641  0.007641  0.489837  0.020163  0.475944  0.014056  \n23  0.559167  0.020833  0.520931  0.010931  0.512768  0.022768  \n21  0.570692  0.009308  0.470860  0.039140  0.470284  0.019716  \n9   0.536665  0.043335  0.530266  0.020266  0.461536  0.028464  \n0   0.569022  0.010978  0.469027  0.040973  0.467583  0.022417  \n22  0.567821  0.012179  0.469417  0.040583  0.469600  0.020400  \n17  0.613623  0.033623  0.597015  0.087015  0.580290  0.090290  \n11  0.572711  0.007289  0.582861  0.072861  0.570164  0.080164  \n20  0.505611  0.074389  0.496701  0.013299  0.491095  0.001095  \n4   0.527494  0.052506  0.442191  0.067809  0.440744  0.049256  \n5   0.526577  0.053423  0.440680  0.069320  0.439193  0.050807  \n2   0.504815  0.075185  0.456370  0.053630  0.459009  0.030991  \n26  0.527099  0.052901  0.448595  0.061405  0.430639  0.059361  \n12  0.634343  0.054343  0.600308  0.090308  0.547630  0.057630  \n8   0.523029  0.056971  0.448561  0.061439  0.429339  0.060661  \n13  0.660206  0.080206  0.600388  0.090388  0.573024  0.083024  \n10  0.632329  0.052329  0.591917  0.081917  0.570442  0.080442  \n1   0.494177  0.085823  0.457482  0.052518  0.455824  0.034176  \n3   0.501994  0.078006  0.437240  0.072760  0.428665  0.061335  \n19  0.472967  0.107033  0.466673  0.043327  0.445900  0.044100  \n6   0.517919  0.062081  0.447699  0.062301  0.427248  0.062752  \n14  0.659317  0.079317  0.636095  0.126095  0.615659  0.125659  \n25  0.486657  0.093343  0.425377  0.084623  0.419890  0.070110  \n7   0.483077  0.096923  0.424013  0.085987  0.419519  0.070481  \n24  0.484645  0.095355  0.401490  0.108510  0.401539  0.088461  \n15  0.453959  0.126041  0.360967  0.149033  0.362035  0.127965  \n16  0.402565  0.177435  0.355542  0.154458  0.383073  0.106927  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dataset_id</th>\n      <th>avg_error</th>\n      <th>length</th>\n      <th>scores1</th>\n      <th>errors1</th>\n      <th>scores2</th>\n      <th>errors2</th>\n      <th>scores3</th>\n      <th>errors3</th>\n      <th>scores4</th>\n      <th>errors4</th>\n      <th>scores5</th>\n      <th>errors5</th>\n      <th>scores6</th>\n      <th>errors6</th>\n      <th>scores7</th>\n      <th>errors7</th>\n      <th>scores8</th>\n      <th>errors8</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>18</th>\n      <td>kishanvavdara</td>\n      <td>0.020993</td>\n      <td>180</td>\n      <td>0.649414</td>\n      <td>0.019414</td>\n      <td>0.630006</td>\n      <td>0.010006</td>\n      <td>0.632212</td>\n      <td>0.022212</td>\n      <td>0.606695</td>\n      <td>0.006695</td>\n      <td>0.604950</td>\n      <td>0.014950</td>\n      <td>0.603744</td>\n      <td>0.023744</td>\n      <td>0.534911</td>\n      <td>0.024911</td>\n      <td>0.536016</td>\n      <td>0.046016</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>winddude</td>\n      <td>0.024018</td>\n      <td>62852</td>\n      <td>0.595853</td>\n      <td>0.034147</td>\n      <td>0.575326</td>\n      <td>0.044674</td>\n      <td>0.560364</td>\n      <td>0.049636</td>\n      <td>0.610081</td>\n      <td>0.010081</td>\n      <td>0.601749</td>\n      <td>0.011749</td>\n      <td>0.587641</td>\n      <td>0.007641</td>\n      <td>0.489837</td>\n      <td>0.020163</td>\n      <td>0.475944</td>\n      <td>0.014056</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>newtonbaba12345_3</td>\n      <td>0.026929</td>\n      <td>1795</td>\n      <td>0.592832</td>\n      <td>0.037168</td>\n      <td>0.576340</td>\n      <td>0.043660</td>\n      <td>0.572451</td>\n      <td>0.037549</td>\n      <td>0.564640</td>\n      <td>0.035360</td>\n      <td>0.582835</td>\n      <td>0.007165</td>\n      <td>0.559167</td>\n      <td>0.020833</td>\n      <td>0.520931</td>\n      <td>0.010931</td>\n      <td>0.512768</td>\n      <td>0.022768</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>newtonbaba12345_1</td>\n      <td>0.032231</td>\n      <td>1000</td>\n      <td>0.587552</td>\n      <td>0.042448</td>\n      <td>0.552869</td>\n      <td>0.067131</td>\n      <td>0.542442</td>\n      <td>0.067558</td>\n      <td>0.595353</td>\n      <td>0.004647</td>\n      <td>0.582101</td>\n      <td>0.007899</td>\n      <td>0.570692</td>\n      <td>0.009308</td>\n      <td>0.470860</td>\n      <td>0.039140</td>\n      <td>0.470284</td>\n      <td>0.019716</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>host</td>\n      <td>0.033217</td>\n      <td>1</td>\n      <td>0.587403</td>\n      <td>0.042597</td>\n      <td>0.598599</td>\n      <td>0.021401</td>\n      <td>0.570934</td>\n      <td>0.039066</td>\n      <td>0.546613</td>\n      <td>0.053387</td>\n      <td>0.572783</td>\n      <td>0.017217</td>\n      <td>0.536665</td>\n      <td>0.043335</td>\n      <td>0.530266</td>\n      <td>0.020266</td>\n      <td>0.461536</td>\n      <td>0.028464</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>aatiffraz</td>\n      <td>0.033696</td>\n      <td>1000</td>\n      <td>0.586325</td>\n      <td>0.043675</td>\n      <td>0.552084</td>\n      <td>0.067916</td>\n      <td>0.541467</td>\n      <td>0.068533</td>\n      <td>0.593825</td>\n      <td>0.006175</td>\n      <td>0.581102</td>\n      <td>0.008898</td>\n      <td>0.569022</td>\n      <td>0.010978</td>\n      <td>0.469027</td>\n      <td>0.040973</td>\n      <td>0.467583</td>\n      <td>0.022417</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>newtonbaba12345_2</td>\n      <td>0.034451</td>\n      <td>1541</td>\n      <td>0.584914</td>\n      <td>0.045086</td>\n      <td>0.550914</td>\n      <td>0.069086</td>\n      <td>0.540632</td>\n      <td>0.069368</td>\n      <td>0.592042</td>\n      <td>0.007958</td>\n      <td>0.579052</td>\n      <td>0.010948</td>\n      <td>0.567821</td>\n      <td>0.012179</td>\n      <td>0.469417</td>\n      <td>0.040583</td>\n      <td>0.469600</td>\n      <td>0.020400</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>juanmerinobermejo</td>\n      <td>0.040164</td>\n      <td>14031</td>\n      <td>0.639861</td>\n      <td>0.009861</td>\n      <td>0.642972</td>\n      <td>0.022972</td>\n      <td>0.633004</td>\n      <td>0.023004</td>\n      <td>0.610931</td>\n      <td>0.010931</td>\n      <td>0.633614</td>\n      <td>0.043614</td>\n      <td>0.613623</td>\n      <td>0.033623</td>\n      <td>0.597015</td>\n      <td>0.087015</td>\n      <td>0.580290</td>\n      <td>0.090290</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>huosiyuan_2</td>\n      <td>0.043354</td>\n      <td>100</td>\n      <td>0.620142</td>\n      <td>0.009858</td>\n      <td>0.685030</td>\n      <td>0.065030</td>\n      <td>0.688003</td>\n      <td>0.078003</td>\n      <td>0.575344</td>\n      <td>0.024656</td>\n      <td>0.581030</td>\n      <td>0.008970</td>\n      <td>0.572711</td>\n      <td>0.007289</td>\n      <td>0.582861</td>\n      <td>0.072861</td>\n      <td>0.570164</td>\n      <td>0.080164</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>nbroad_2</td>\n      <td>0.067890</td>\n      <td>2400</td>\n      <td>0.548175</td>\n      <td>0.081825</td>\n      <td>0.502295</td>\n      <td>0.117705</td>\n      <td>0.501688</td>\n      <td>0.108312</td>\n      <td>0.509477</td>\n      <td>0.090523</td>\n      <td>0.534027</td>\n      <td>0.055973</td>\n      <td>0.505611</td>\n      <td>0.074389</td>\n      <td>0.496701</td>\n      <td>0.013299</td>\n      <td>0.491095</td>\n      <td>0.001095</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>alexxxsem_2</td>\n      <td>0.068758</td>\n      <td>3991</td>\n      <td>0.548970</td>\n      <td>0.081030</td>\n      <td>0.520243</td>\n      <td>0.099757</td>\n      <td>0.489936</td>\n      <td>0.120064</td>\n      <td>0.570146</td>\n      <td>0.029854</td>\n      <td>0.540213</td>\n      <td>0.049787</td>\n      <td>0.527494</td>\n      <td>0.052506</td>\n      <td>0.442191</td>\n      <td>0.067809</td>\n      <td>0.440744</td>\n      <td>0.049256</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>alexxxsem_3</td>\n      <td>0.069895</td>\n      <td>5371</td>\n      <td>0.547754</td>\n      <td>0.082246</td>\n      <td>0.519098</td>\n      <td>0.100902</td>\n      <td>0.488494</td>\n      <td>0.121506</td>\n      <td>0.569513</td>\n      <td>0.030487</td>\n      <td>0.539531</td>\n      <td>0.050469</td>\n      <td>0.526577</td>\n      <td>0.053423</td>\n      <td>0.440680</td>\n      <td>0.069320</td>\n      <td>0.439193</td>\n      <td>0.050807</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>aishaalmahmoud_2</td>\n      <td>0.076295</td>\n      <td>15597</td>\n      <td>0.526221</td>\n      <td>0.103779</td>\n      <td>0.514402</td>\n      <td>0.105598</td>\n      <td>0.499269</td>\n      <td>0.110731</td>\n      <td>0.533285</td>\n      <td>0.066715</td>\n      <td>0.526269</td>\n      <td>0.063731</td>\n      <td>0.504815</td>\n      <td>0.075185</td>\n      <td>0.456370</td>\n      <td>0.053630</td>\n      <td>0.459009</td>\n      <td>0.030991</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>thedrcat_3</td>\n      <td>0.079563</td>\n      <td>9140</td>\n      <td>0.514268</td>\n      <td>0.115732</td>\n      <td>0.499633</td>\n      <td>0.120367</td>\n      <td>0.485819</td>\n      <td>0.124181</td>\n      <td>0.558524</td>\n      <td>0.041476</td>\n      <td>0.528921</td>\n      <td>0.061079</td>\n      <td>0.527099</td>\n      <td>0.052901</td>\n      <td>0.448595</td>\n      <td>0.061405</td>\n      <td>0.430639</td>\n      <td>0.059361</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>huosiyuan_3</td>\n      <td>0.082411</td>\n      <td>69</td>\n      <td>0.740298</td>\n      <td>0.110298</td>\n      <td>0.733707</td>\n      <td>0.113707</td>\n      <td>0.747077</td>\n      <td>0.137077</td>\n      <td>0.638633</td>\n      <td>0.038633</td>\n      <td>0.647296</td>\n      <td>0.057296</td>\n      <td>0.634343</td>\n      <td>0.054343</td>\n      <td>0.600308</td>\n      <td>0.090308</td>\n      <td>0.547630</td>\n      <td>0.057630</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>gali1eo</td>\n      <td>0.084614</td>\n      <td>20507</td>\n      <td>0.508259</td>\n      <td>0.121741</td>\n      <td>0.491093</td>\n      <td>0.128907</td>\n      <td>0.478402</td>\n      <td>0.131598</td>\n      <td>0.551151</td>\n      <td>0.048849</td>\n      <td>0.523252</td>\n      <td>0.066748</td>\n      <td>0.523029</td>\n      <td>0.056971</td>\n      <td>0.448561</td>\n      <td>0.061439</td>\n      <td>0.429339</td>\n      <td>0.060661</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>huosiyuan_4</td>\n      <td>0.088523</td>\n      <td>73</td>\n      <td>0.702262</td>\n      <td>0.072262</td>\n      <td>0.741135</td>\n      <td>0.121135</td>\n      <td>0.729188</td>\n      <td>0.119188</td>\n      <td>0.663045</td>\n      <td>0.063045</td>\n      <td>0.668933</td>\n      <td>0.078933</td>\n      <td>0.660206</td>\n      <td>0.080206</td>\n      <td>0.600388</td>\n      <td>0.090388</td>\n      <td>0.573024</td>\n      <td>0.083024</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>huosiyuan_1</td>\n      <td>0.088642</td>\n      <td>376</td>\n      <td>0.716723</td>\n      <td>0.086723</td>\n      <td>0.763613</td>\n      <td>0.143613</td>\n      <td>0.779660</td>\n      <td>0.169660</td>\n      <td>0.638566</td>\n      <td>0.038566</td>\n      <td>0.645883</td>\n      <td>0.055883</td>\n      <td>0.632329</td>\n      <td>0.052329</td>\n      <td>0.591917</td>\n      <td>0.081917</td>\n      <td>0.570442</td>\n      <td>0.080442</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>aishaalmahmoud_1</td>\n      <td>0.089077</td>\n      <td>4234</td>\n      <td>0.500417</td>\n      <td>0.129583</td>\n      <td>0.497672</td>\n      <td>0.122328</td>\n      <td>0.482927</td>\n      <td>0.127073</td>\n      <td>0.523195</td>\n      <td>0.076805</td>\n      <td>0.505694</td>\n      <td>0.084306</td>\n      <td>0.494177</td>\n      <td>0.085823</td>\n      <td>0.457482</td>\n      <td>0.052518</td>\n      <td>0.455824</td>\n      <td>0.034176</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>alexxxsem_1</td>\n      <td>0.089292</td>\n      <td>224</td>\n      <td>0.516868</td>\n      <td>0.113132</td>\n      <td>0.498966</td>\n      <td>0.121034</td>\n      <td>0.469127</td>\n      <td>0.140873</td>\n      <td>0.551874</td>\n      <td>0.048126</td>\n      <td>0.510928</td>\n      <td>0.079072</td>\n      <td>0.501994</td>\n      <td>0.078006</td>\n      <td>0.437240</td>\n      <td>0.072760</td>\n      <td>0.428665</td>\n      <td>0.061335</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>nbroad_1</td>\n      <td>0.090520</td>\n      <td>2166</td>\n      <td>0.522781</td>\n      <td>0.107219</td>\n      <td>0.510654</td>\n      <td>0.109346</td>\n      <td>0.513224</td>\n      <td>0.096776</td>\n      <td>0.487186</td>\n      <td>0.112814</td>\n      <td>0.486453</td>\n      <td>0.103547</td>\n      <td>0.472967</td>\n      <td>0.107033</td>\n      <td>0.466673</td>\n      <td>0.043327</td>\n      <td>0.445900</td>\n      <td>0.044100</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>dipamc77</td>\n      <td>0.090866</td>\n      <td>2004</td>\n      <td>0.502789</td>\n      <td>0.127211</td>\n      <td>0.481107</td>\n      <td>0.138893</td>\n      <td>0.469855</td>\n      <td>0.140145</td>\n      <td>0.541341</td>\n      <td>0.058659</td>\n      <td>0.515115</td>\n      <td>0.074885</td>\n      <td>0.517919</td>\n      <td>0.062081</td>\n      <td>0.447699</td>\n      <td>0.062301</td>\n      <td>0.427248</td>\n      <td>0.062752</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>huosiyuan_5</td>\n      <td>0.106236</td>\n      <td>44</td>\n      <td>0.702661</td>\n      <td>0.072661</td>\n      <td>0.761279</td>\n      <td>0.141279</td>\n      <td>0.764627</td>\n      <td>0.154627</td>\n      <td>0.655224</td>\n      <td>0.055224</td>\n      <td>0.685028</td>\n      <td>0.095028</td>\n      <td>0.659317</td>\n      <td>0.079317</td>\n      <td>0.636095</td>\n      <td>0.126095</td>\n      <td>0.615659</td>\n      <td>0.125659</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>thedrcat_2</td>\n      <td>0.108396</td>\n      <td>860</td>\n      <td>0.482486</td>\n      <td>0.147514</td>\n      <td>0.472132</td>\n      <td>0.147868</td>\n      <td>0.451916</td>\n      <td>0.158084</td>\n      <td>0.530429</td>\n      <td>0.069571</td>\n      <td>0.493945</td>\n      <td>0.096055</td>\n      <td>0.486657</td>\n      <td>0.093343</td>\n      <td>0.425377</td>\n      <td>0.084623</td>\n      <td>0.419890</td>\n      <td>0.070110</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>dschettler8845</td>\n      <td>0.110380</td>\n      <td>1000</td>\n      <td>0.480585</td>\n      <td>0.149415</td>\n      <td>0.471443</td>\n      <td>0.148557</td>\n      <td>0.450902</td>\n      <td>0.159098</td>\n      <td>0.526768</td>\n      <td>0.073232</td>\n      <td>0.490650</td>\n      <td>0.099350</td>\n      <td>0.483077</td>\n      <td>0.096923</td>\n      <td>0.424013</td>\n      <td>0.085987</td>\n      <td>0.419519</td>\n      <td>0.070481</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>thedrcat_1</td>\n      <td>0.117457</td>\n      <td>89</td>\n      <td>0.482441</td>\n      <td>0.147559</td>\n      <td>0.457324</td>\n      <td>0.162676</td>\n      <td>0.438741</td>\n      <td>0.171259</td>\n      <td>0.531514</td>\n      <td>0.068486</td>\n      <td>0.492646</td>\n      <td>0.097354</td>\n      <td>0.484645</td>\n      <td>0.095355</td>\n      <td>0.401490</td>\n      <td>0.108510</td>\n      <td>0.401539</td>\n      <td>0.088461</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>isakatsuyoshi_1</td>\n      <td>0.151681</td>\n      <td>5</td>\n      <td>0.450158</td>\n      <td>0.179842</td>\n      <td>0.425723</td>\n      <td>0.194277</td>\n      <td>0.418465</td>\n      <td>0.191535</td>\n      <td>0.488909</td>\n      <td>0.111091</td>\n      <td>0.456336</td>\n      <td>0.133664</td>\n      <td>0.453959</td>\n      <td>0.126041</td>\n      <td>0.360967</td>\n      <td>0.149033</td>\n      <td>0.362035</td>\n      <td>0.127965</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>isakatsuyoshi_2</td>\n      <td>0.158763</td>\n      <td>3</td>\n      <td>0.465808</td>\n      <td>0.164192</td>\n      <td>0.438457</td>\n      <td>0.181543</td>\n      <td>0.403398</td>\n      <td>0.206602</td>\n      <td>0.466924</td>\n      <td>0.133076</td>\n      <td>0.444128</td>\n      <td>0.145872</td>\n      <td>0.402565</td>\n      <td>0.177435</td>\n      <td>0.355542</td>\n      <td>0.154458</td>\n      <td>0.383073</td>\n      <td>0.106927</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"acc_errors = []\nacc_len = 0\nacc_error = 0\nfor idx, row in scores.iterrows():\n    acc_len += row['length']\n    acc_error += row['length'] * row['avg_error']\n    acc_errors.append(acc_error/acc_len)\nscores['acc_error'] = acc_errors\ndisplay(scores)\nprint(scores.query(\"acc_error<0.03\").dataset_id)\n\n# selected_ds = scores.query(\"avg_error<0.04\").dataset_id\n# selected_emb = validation_df[validation_df['dataset_id'].isin(selected_ds)].filter(like=\"rewrite_prompt_emb\")\n# preds_embedded = model.encode(rewrite_prompts[0], normalize_embeddings=True, show_progress_bar=True)\n# score = (cosine_similarity(selected_emb, preds_embedded.reshape(1,-1))**3).mean()\n# print(score)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T16:13:36.767486Z","iopub.execute_input":"2024-04-15T16:13:36.768195Z","iopub.status.idle":"2024-04-15T16:13:36.816619Z","shell.execute_reply.started":"2024-04-15T16:13:36.768162Z","shell.execute_reply":"2024-04-15T16:13:36.815697Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"           dataset_id  avg_error  length   scores1   errors1   scores2  \\\n18      kishanvavdara   0.020993     180  0.649414  0.019414  0.630006   \n27           winddude   0.024018   62852  0.595853  0.034147  0.575326   \n23  newtonbaba12345_3   0.026929    1795  0.592832  0.037168  0.576340   \n21  newtonbaba12345_1   0.032231    1000  0.587552  0.042448  0.552869   \n9                host   0.033217       1  0.587403  0.042597  0.598599   \n0           aatiffraz   0.033696    1000  0.586325  0.043675  0.552084   \n22  newtonbaba12345_2   0.034451    1541  0.584914  0.045086  0.550914   \n17  juanmerinobermejo   0.040164   14031  0.639861  0.009861  0.642972   \n11        huosiyuan_2   0.043354     100  0.620142  0.009858  0.685030   \n20           nbroad_2   0.067890    2400  0.548175  0.081825  0.502295   \n4         alexxxsem_2   0.068758    3991  0.548970  0.081030  0.520243   \n5         alexxxsem_3   0.069895    5371  0.547754  0.082246  0.519098   \n2    aishaalmahmoud_2   0.076295   15597  0.526221  0.103779  0.514402   \n26         thedrcat_3   0.079563    9140  0.514268  0.115732  0.499633   \n12        huosiyuan_3   0.082411      69  0.740298  0.110298  0.733707   \n8             gali1eo   0.084614   20507  0.508259  0.121741  0.491093   \n13        huosiyuan_4   0.088523      73  0.702262  0.072262  0.741135   \n10        huosiyuan_1   0.088642     376  0.716723  0.086723  0.763613   \n1    aishaalmahmoud_1   0.089077    4234  0.500417  0.129583  0.497672   \n3         alexxxsem_1   0.089292     224  0.516868  0.113132  0.498966   \n19           nbroad_1   0.090520    2166  0.522781  0.107219  0.510654   \n6            dipamc77   0.090866    2004  0.502789  0.127211  0.481107   \n14        huosiyuan_5   0.106236      44  0.702661  0.072661  0.761279   \n25         thedrcat_2   0.108396     860  0.482486  0.147514  0.472132   \n7      dschettler8845   0.110380    1000  0.480585  0.149415  0.471443   \n24         thedrcat_1   0.117457      89  0.482441  0.147559  0.457324   \n15    isakatsuyoshi_1   0.151681       5  0.450158  0.179842  0.425723   \n16    isakatsuyoshi_2   0.158763       3  0.465808  0.164192  0.438457   \n\n     errors2   scores3   errors3   scores4   errors4   scores5   errors5  \\\n18  0.010006  0.632212  0.022212  0.606695  0.006695  0.604950  0.014950   \n27  0.044674  0.560364  0.049636  0.610081  0.010081  0.601749  0.011749   \n23  0.043660  0.572451  0.037549  0.564640  0.035360  0.582835  0.007165   \n21  0.067131  0.542442  0.067558  0.595353  0.004647  0.582101  0.007899   \n9   0.021401  0.570934  0.039066  0.546613  0.053387  0.572783  0.017217   \n0   0.067916  0.541467  0.068533  0.593825  0.006175  0.581102  0.008898   \n22  0.069086  0.540632  0.069368  0.592042  0.007958  0.579052  0.010948   \n17  0.022972  0.633004  0.023004  0.610931  0.010931  0.633614  0.043614   \n11  0.065030  0.688003  0.078003  0.575344  0.024656  0.581030  0.008970   \n20  0.117705  0.501688  0.108312  0.509477  0.090523  0.534027  0.055973   \n4   0.099757  0.489936  0.120064  0.570146  0.029854  0.540213  0.049787   \n5   0.100902  0.488494  0.121506  0.569513  0.030487  0.539531  0.050469   \n2   0.105598  0.499269  0.110731  0.533285  0.066715  0.526269  0.063731   \n26  0.120367  0.485819  0.124181  0.558524  0.041476  0.528921  0.061079   \n12  0.113707  0.747077  0.137077  0.638633  0.038633  0.647296  0.057296   \n8   0.128907  0.478402  0.131598  0.551151  0.048849  0.523252  0.066748   \n13  0.121135  0.729188  0.119188  0.663045  0.063045  0.668933  0.078933   \n10  0.143613  0.779660  0.169660  0.638566  0.038566  0.645883  0.055883   \n1   0.122328  0.482927  0.127073  0.523195  0.076805  0.505694  0.084306   \n3   0.121034  0.469127  0.140873  0.551874  0.048126  0.510928  0.079072   \n19  0.109346  0.513224  0.096776  0.487186  0.112814  0.486453  0.103547   \n6   0.138893  0.469855  0.140145  0.541341  0.058659  0.515115  0.074885   \n14  0.141279  0.764627  0.154627  0.655224  0.055224  0.685028  0.095028   \n25  0.147868  0.451916  0.158084  0.530429  0.069571  0.493945  0.096055   \n7   0.148557  0.450902  0.159098  0.526768  0.073232  0.490650  0.099350   \n24  0.162676  0.438741  0.171259  0.531514  0.068486  0.492646  0.097354   \n15  0.194277  0.418465  0.191535  0.488909  0.111091  0.456336  0.133664   \n16  0.181543  0.403398  0.206602  0.466924  0.133076  0.444128  0.145872   \n\n     scores6   errors6   scores7   errors7   scores8   errors8  acc_error  \n18  0.603744  0.023744  0.534911  0.024911  0.536016  0.046016   0.020993  \n27  0.587641  0.007641  0.489837  0.020163  0.475944  0.014056   0.024010  \n23  0.559167  0.020833  0.520931  0.010931  0.512768  0.022768   0.024090  \n21  0.570692  0.009308  0.470860  0.039140  0.470284  0.019716   0.024214  \n9   0.536665  0.043335  0.530266  0.020266  0.461536  0.028464   0.024214  \n0   0.569022  0.010978  0.469027  0.040973  0.467583  0.022417   0.024356  \n22  0.567821  0.012179  0.469417  0.040583  0.469600  0.020400   0.024584  \n17  0.613623  0.033623  0.597015  0.087015  0.580290  0.090290   0.027237  \n11  0.572711  0.007289  0.582861  0.072861  0.570164  0.080164   0.027256  \n20  0.505611  0.074389  0.496701  0.013299  0.491095  0.001095   0.028405  \n4   0.527494  0.052506  0.442191  0.067809  0.440744  0.049256   0.030217  \n5   0.526577  0.053423  0.440680  0.069320  0.439193  0.050807   0.032477  \n2   0.504815  0.075185  0.456370  0.053630  0.459009  0.030991   0.038698  \n26  0.527099  0.052901  0.448595  0.061405  0.430639  0.059361   0.041837  \n12  0.634343  0.054343  0.600308  0.090308  0.547630  0.057630   0.041861  \n8   0.523029  0.056971  0.448561  0.061439  0.429339  0.060661   0.048142  \n13  0.660206  0.080206  0.600388  0.090388  0.573024  0.083024   0.048163  \n10  0.632329  0.052329  0.591917  0.081917  0.570442  0.080442   0.048272  \n1   0.494177  0.085823  0.457482  0.052518  0.455824  0.034176   0.049470  \n3   0.501994  0.078006  0.437240  0.072760  0.428665  0.061335   0.049531  \n19  0.472967  0.107033  0.466673  0.043327  0.445900  0.044100   0.050137  \n6   0.517919  0.062081  0.447699  0.062301  0.427248  0.062752   0.050686  \n14  0.659317  0.079317  0.636095  0.126095  0.615659  0.125659   0.050702  \n25  0.486657  0.093343  0.425377  0.084623  0.419890  0.070110   0.051034  \n7   0.483077  0.096923  0.424013  0.085987  0.419519  0.070481   0.051428  \n24  0.484645  0.095355  0.401490  0.108510  0.401539  0.088461   0.051467  \n15  0.453959  0.126041  0.360967  0.149033  0.362035  0.127965   0.051471  \n16  0.402565  0.177435  0.355542  0.154458  0.383073  0.106927   0.051473  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dataset_id</th>\n      <th>avg_error</th>\n      <th>length</th>\n      <th>scores1</th>\n      <th>errors1</th>\n      <th>scores2</th>\n      <th>errors2</th>\n      <th>scores3</th>\n      <th>errors3</th>\n      <th>scores4</th>\n      <th>errors4</th>\n      <th>scores5</th>\n      <th>errors5</th>\n      <th>scores6</th>\n      <th>errors6</th>\n      <th>scores7</th>\n      <th>errors7</th>\n      <th>scores8</th>\n      <th>errors8</th>\n      <th>acc_error</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>18</th>\n      <td>kishanvavdara</td>\n      <td>0.020993</td>\n      <td>180</td>\n      <td>0.649414</td>\n      <td>0.019414</td>\n      <td>0.630006</td>\n      <td>0.010006</td>\n      <td>0.632212</td>\n      <td>0.022212</td>\n      <td>0.606695</td>\n      <td>0.006695</td>\n      <td>0.604950</td>\n      <td>0.014950</td>\n      <td>0.603744</td>\n      <td>0.023744</td>\n      <td>0.534911</td>\n      <td>0.024911</td>\n      <td>0.536016</td>\n      <td>0.046016</td>\n      <td>0.020993</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>winddude</td>\n      <td>0.024018</td>\n      <td>62852</td>\n      <td>0.595853</td>\n      <td>0.034147</td>\n      <td>0.575326</td>\n      <td>0.044674</td>\n      <td>0.560364</td>\n      <td>0.049636</td>\n      <td>0.610081</td>\n      <td>0.010081</td>\n      <td>0.601749</td>\n      <td>0.011749</td>\n      <td>0.587641</td>\n      <td>0.007641</td>\n      <td>0.489837</td>\n      <td>0.020163</td>\n      <td>0.475944</td>\n      <td>0.014056</td>\n      <td>0.024010</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>newtonbaba12345_3</td>\n      <td>0.026929</td>\n      <td>1795</td>\n      <td>0.592832</td>\n      <td>0.037168</td>\n      <td>0.576340</td>\n      <td>0.043660</td>\n      <td>0.572451</td>\n      <td>0.037549</td>\n      <td>0.564640</td>\n      <td>0.035360</td>\n      <td>0.582835</td>\n      <td>0.007165</td>\n      <td>0.559167</td>\n      <td>0.020833</td>\n      <td>0.520931</td>\n      <td>0.010931</td>\n      <td>0.512768</td>\n      <td>0.022768</td>\n      <td>0.024090</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>newtonbaba12345_1</td>\n      <td>0.032231</td>\n      <td>1000</td>\n      <td>0.587552</td>\n      <td>0.042448</td>\n      <td>0.552869</td>\n      <td>0.067131</td>\n      <td>0.542442</td>\n      <td>0.067558</td>\n      <td>0.595353</td>\n      <td>0.004647</td>\n      <td>0.582101</td>\n      <td>0.007899</td>\n      <td>0.570692</td>\n      <td>0.009308</td>\n      <td>0.470860</td>\n      <td>0.039140</td>\n      <td>0.470284</td>\n      <td>0.019716</td>\n      <td>0.024214</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>host</td>\n      <td>0.033217</td>\n      <td>1</td>\n      <td>0.587403</td>\n      <td>0.042597</td>\n      <td>0.598599</td>\n      <td>0.021401</td>\n      <td>0.570934</td>\n      <td>0.039066</td>\n      <td>0.546613</td>\n      <td>0.053387</td>\n      <td>0.572783</td>\n      <td>0.017217</td>\n      <td>0.536665</td>\n      <td>0.043335</td>\n      <td>0.530266</td>\n      <td>0.020266</td>\n      <td>0.461536</td>\n      <td>0.028464</td>\n      <td>0.024214</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>aatiffraz</td>\n      <td>0.033696</td>\n      <td>1000</td>\n      <td>0.586325</td>\n      <td>0.043675</td>\n      <td>0.552084</td>\n      <td>0.067916</td>\n      <td>0.541467</td>\n      <td>0.068533</td>\n      <td>0.593825</td>\n      <td>0.006175</td>\n      <td>0.581102</td>\n      <td>0.008898</td>\n      <td>0.569022</td>\n      <td>0.010978</td>\n      <td>0.469027</td>\n      <td>0.040973</td>\n      <td>0.467583</td>\n      <td>0.022417</td>\n      <td>0.024356</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>newtonbaba12345_2</td>\n      <td>0.034451</td>\n      <td>1541</td>\n      <td>0.584914</td>\n      <td>0.045086</td>\n      <td>0.550914</td>\n      <td>0.069086</td>\n      <td>0.540632</td>\n      <td>0.069368</td>\n      <td>0.592042</td>\n      <td>0.007958</td>\n      <td>0.579052</td>\n      <td>0.010948</td>\n      <td>0.567821</td>\n      <td>0.012179</td>\n      <td>0.469417</td>\n      <td>0.040583</td>\n      <td>0.469600</td>\n      <td>0.020400</td>\n      <td>0.024584</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>juanmerinobermejo</td>\n      <td>0.040164</td>\n      <td>14031</td>\n      <td>0.639861</td>\n      <td>0.009861</td>\n      <td>0.642972</td>\n      <td>0.022972</td>\n      <td>0.633004</td>\n      <td>0.023004</td>\n      <td>0.610931</td>\n      <td>0.010931</td>\n      <td>0.633614</td>\n      <td>0.043614</td>\n      <td>0.613623</td>\n      <td>0.033623</td>\n      <td>0.597015</td>\n      <td>0.087015</td>\n      <td>0.580290</td>\n      <td>0.090290</td>\n      <td>0.027237</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>huosiyuan_2</td>\n      <td>0.043354</td>\n      <td>100</td>\n      <td>0.620142</td>\n      <td>0.009858</td>\n      <td>0.685030</td>\n      <td>0.065030</td>\n      <td>0.688003</td>\n      <td>0.078003</td>\n      <td>0.575344</td>\n      <td>0.024656</td>\n      <td>0.581030</td>\n      <td>0.008970</td>\n      <td>0.572711</td>\n      <td>0.007289</td>\n      <td>0.582861</td>\n      <td>0.072861</td>\n      <td>0.570164</td>\n      <td>0.080164</td>\n      <td>0.027256</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>nbroad_2</td>\n      <td>0.067890</td>\n      <td>2400</td>\n      <td>0.548175</td>\n      <td>0.081825</td>\n      <td>0.502295</td>\n      <td>0.117705</td>\n      <td>0.501688</td>\n      <td>0.108312</td>\n      <td>0.509477</td>\n      <td>0.090523</td>\n      <td>0.534027</td>\n      <td>0.055973</td>\n      <td>0.505611</td>\n      <td>0.074389</td>\n      <td>0.496701</td>\n      <td>0.013299</td>\n      <td>0.491095</td>\n      <td>0.001095</td>\n      <td>0.028405</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>alexxxsem_2</td>\n      <td>0.068758</td>\n      <td>3991</td>\n      <td>0.548970</td>\n      <td>0.081030</td>\n      <td>0.520243</td>\n      <td>0.099757</td>\n      <td>0.489936</td>\n      <td>0.120064</td>\n      <td>0.570146</td>\n      <td>0.029854</td>\n      <td>0.540213</td>\n      <td>0.049787</td>\n      <td>0.527494</td>\n      <td>0.052506</td>\n      <td>0.442191</td>\n      <td>0.067809</td>\n      <td>0.440744</td>\n      <td>0.049256</td>\n      <td>0.030217</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>alexxxsem_3</td>\n      <td>0.069895</td>\n      <td>5371</td>\n      <td>0.547754</td>\n      <td>0.082246</td>\n      <td>0.519098</td>\n      <td>0.100902</td>\n      <td>0.488494</td>\n      <td>0.121506</td>\n      <td>0.569513</td>\n      <td>0.030487</td>\n      <td>0.539531</td>\n      <td>0.050469</td>\n      <td>0.526577</td>\n      <td>0.053423</td>\n      <td>0.440680</td>\n      <td>0.069320</td>\n      <td>0.439193</td>\n      <td>0.050807</td>\n      <td>0.032477</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>aishaalmahmoud_2</td>\n      <td>0.076295</td>\n      <td>15597</td>\n      <td>0.526221</td>\n      <td>0.103779</td>\n      <td>0.514402</td>\n      <td>0.105598</td>\n      <td>0.499269</td>\n      <td>0.110731</td>\n      <td>0.533285</td>\n      <td>0.066715</td>\n      <td>0.526269</td>\n      <td>0.063731</td>\n      <td>0.504815</td>\n      <td>0.075185</td>\n      <td>0.456370</td>\n      <td>0.053630</td>\n      <td>0.459009</td>\n      <td>0.030991</td>\n      <td>0.038698</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>thedrcat_3</td>\n      <td>0.079563</td>\n      <td>9140</td>\n      <td>0.514268</td>\n      <td>0.115732</td>\n      <td>0.499633</td>\n      <td>0.120367</td>\n      <td>0.485819</td>\n      <td>0.124181</td>\n      <td>0.558524</td>\n      <td>0.041476</td>\n      <td>0.528921</td>\n      <td>0.061079</td>\n      <td>0.527099</td>\n      <td>0.052901</td>\n      <td>0.448595</td>\n      <td>0.061405</td>\n      <td>0.430639</td>\n      <td>0.059361</td>\n      <td>0.041837</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>huosiyuan_3</td>\n      <td>0.082411</td>\n      <td>69</td>\n      <td>0.740298</td>\n      <td>0.110298</td>\n      <td>0.733707</td>\n      <td>0.113707</td>\n      <td>0.747077</td>\n      <td>0.137077</td>\n      <td>0.638633</td>\n      <td>0.038633</td>\n      <td>0.647296</td>\n      <td>0.057296</td>\n      <td>0.634343</td>\n      <td>0.054343</td>\n      <td>0.600308</td>\n      <td>0.090308</td>\n      <td>0.547630</td>\n      <td>0.057630</td>\n      <td>0.041861</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>gali1eo</td>\n      <td>0.084614</td>\n      <td>20507</td>\n      <td>0.508259</td>\n      <td>0.121741</td>\n      <td>0.491093</td>\n      <td>0.128907</td>\n      <td>0.478402</td>\n      <td>0.131598</td>\n      <td>0.551151</td>\n      <td>0.048849</td>\n      <td>0.523252</td>\n      <td>0.066748</td>\n      <td>0.523029</td>\n      <td>0.056971</td>\n      <td>0.448561</td>\n      <td>0.061439</td>\n      <td>0.429339</td>\n      <td>0.060661</td>\n      <td>0.048142</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>huosiyuan_4</td>\n      <td>0.088523</td>\n      <td>73</td>\n      <td>0.702262</td>\n      <td>0.072262</td>\n      <td>0.741135</td>\n      <td>0.121135</td>\n      <td>0.729188</td>\n      <td>0.119188</td>\n      <td>0.663045</td>\n      <td>0.063045</td>\n      <td>0.668933</td>\n      <td>0.078933</td>\n      <td>0.660206</td>\n      <td>0.080206</td>\n      <td>0.600388</td>\n      <td>0.090388</td>\n      <td>0.573024</td>\n      <td>0.083024</td>\n      <td>0.048163</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>huosiyuan_1</td>\n      <td>0.088642</td>\n      <td>376</td>\n      <td>0.716723</td>\n      <td>0.086723</td>\n      <td>0.763613</td>\n      <td>0.143613</td>\n      <td>0.779660</td>\n      <td>0.169660</td>\n      <td>0.638566</td>\n      <td>0.038566</td>\n      <td>0.645883</td>\n      <td>0.055883</td>\n      <td>0.632329</td>\n      <td>0.052329</td>\n      <td>0.591917</td>\n      <td>0.081917</td>\n      <td>0.570442</td>\n      <td>0.080442</td>\n      <td>0.048272</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>aishaalmahmoud_1</td>\n      <td>0.089077</td>\n      <td>4234</td>\n      <td>0.500417</td>\n      <td>0.129583</td>\n      <td>0.497672</td>\n      <td>0.122328</td>\n      <td>0.482927</td>\n      <td>0.127073</td>\n      <td>0.523195</td>\n      <td>0.076805</td>\n      <td>0.505694</td>\n      <td>0.084306</td>\n      <td>0.494177</td>\n      <td>0.085823</td>\n      <td>0.457482</td>\n      <td>0.052518</td>\n      <td>0.455824</td>\n      <td>0.034176</td>\n      <td>0.049470</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>alexxxsem_1</td>\n      <td>0.089292</td>\n      <td>224</td>\n      <td>0.516868</td>\n      <td>0.113132</td>\n      <td>0.498966</td>\n      <td>0.121034</td>\n      <td>0.469127</td>\n      <td>0.140873</td>\n      <td>0.551874</td>\n      <td>0.048126</td>\n      <td>0.510928</td>\n      <td>0.079072</td>\n      <td>0.501994</td>\n      <td>0.078006</td>\n      <td>0.437240</td>\n      <td>0.072760</td>\n      <td>0.428665</td>\n      <td>0.061335</td>\n      <td>0.049531</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>nbroad_1</td>\n      <td>0.090520</td>\n      <td>2166</td>\n      <td>0.522781</td>\n      <td>0.107219</td>\n      <td>0.510654</td>\n      <td>0.109346</td>\n      <td>0.513224</td>\n      <td>0.096776</td>\n      <td>0.487186</td>\n      <td>0.112814</td>\n      <td>0.486453</td>\n      <td>0.103547</td>\n      <td>0.472967</td>\n      <td>0.107033</td>\n      <td>0.466673</td>\n      <td>0.043327</td>\n      <td>0.445900</td>\n      <td>0.044100</td>\n      <td>0.050137</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>dipamc77</td>\n      <td>0.090866</td>\n      <td>2004</td>\n      <td>0.502789</td>\n      <td>0.127211</td>\n      <td>0.481107</td>\n      <td>0.138893</td>\n      <td>0.469855</td>\n      <td>0.140145</td>\n      <td>0.541341</td>\n      <td>0.058659</td>\n      <td>0.515115</td>\n      <td>0.074885</td>\n      <td>0.517919</td>\n      <td>0.062081</td>\n      <td>0.447699</td>\n      <td>0.062301</td>\n      <td>0.427248</td>\n      <td>0.062752</td>\n      <td>0.050686</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>huosiyuan_5</td>\n      <td>0.106236</td>\n      <td>44</td>\n      <td>0.702661</td>\n      <td>0.072661</td>\n      <td>0.761279</td>\n      <td>0.141279</td>\n      <td>0.764627</td>\n      <td>0.154627</td>\n      <td>0.655224</td>\n      <td>0.055224</td>\n      <td>0.685028</td>\n      <td>0.095028</td>\n      <td>0.659317</td>\n      <td>0.079317</td>\n      <td>0.636095</td>\n      <td>0.126095</td>\n      <td>0.615659</td>\n      <td>0.125659</td>\n      <td>0.050702</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>thedrcat_2</td>\n      <td>0.108396</td>\n      <td>860</td>\n      <td>0.482486</td>\n      <td>0.147514</td>\n      <td>0.472132</td>\n      <td>0.147868</td>\n      <td>0.451916</td>\n      <td>0.158084</td>\n      <td>0.530429</td>\n      <td>0.069571</td>\n      <td>0.493945</td>\n      <td>0.096055</td>\n      <td>0.486657</td>\n      <td>0.093343</td>\n      <td>0.425377</td>\n      <td>0.084623</td>\n      <td>0.419890</td>\n      <td>0.070110</td>\n      <td>0.051034</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>dschettler8845</td>\n      <td>0.110380</td>\n      <td>1000</td>\n      <td>0.480585</td>\n      <td>0.149415</td>\n      <td>0.471443</td>\n      <td>0.148557</td>\n      <td>0.450902</td>\n      <td>0.159098</td>\n      <td>0.526768</td>\n      <td>0.073232</td>\n      <td>0.490650</td>\n      <td>0.099350</td>\n      <td>0.483077</td>\n      <td>0.096923</td>\n      <td>0.424013</td>\n      <td>0.085987</td>\n      <td>0.419519</td>\n      <td>0.070481</td>\n      <td>0.051428</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>thedrcat_1</td>\n      <td>0.117457</td>\n      <td>89</td>\n      <td>0.482441</td>\n      <td>0.147559</td>\n      <td>0.457324</td>\n      <td>0.162676</td>\n      <td>0.438741</td>\n      <td>0.171259</td>\n      <td>0.531514</td>\n      <td>0.068486</td>\n      <td>0.492646</td>\n      <td>0.097354</td>\n      <td>0.484645</td>\n      <td>0.095355</td>\n      <td>0.401490</td>\n      <td>0.108510</td>\n      <td>0.401539</td>\n      <td>0.088461</td>\n      <td>0.051467</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>isakatsuyoshi_1</td>\n      <td>0.151681</td>\n      <td>5</td>\n      <td>0.450158</td>\n      <td>0.179842</td>\n      <td>0.425723</td>\n      <td>0.194277</td>\n      <td>0.418465</td>\n      <td>0.191535</td>\n      <td>0.488909</td>\n      <td>0.111091</td>\n      <td>0.456336</td>\n      <td>0.133664</td>\n      <td>0.453959</td>\n      <td>0.126041</td>\n      <td>0.360967</td>\n      <td>0.149033</td>\n      <td>0.362035</td>\n      <td>0.127965</td>\n      <td>0.051471</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>isakatsuyoshi_2</td>\n      <td>0.158763</td>\n      <td>3</td>\n      <td>0.465808</td>\n      <td>0.164192</td>\n      <td>0.438457</td>\n      <td>0.181543</td>\n      <td>0.403398</td>\n      <td>0.206602</td>\n      <td>0.466924</td>\n      <td>0.133076</td>\n      <td>0.444128</td>\n      <td>0.145872</td>\n      <td>0.402565</td>\n      <td>0.177435</td>\n      <td>0.355542</td>\n      <td>0.154458</td>\n      <td>0.383073</td>\n      <td>0.106927</td>\n      <td>0.051473</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"18        kishanvavdara\n27             winddude\n23    newtonbaba12345_3\n21    newtonbaba12345_1\n9                  host\n0             aatiffraz\n22    newtonbaba12345_2\n17    juanmerinobermejo\n11          huosiyuan_2\n20             nbroad_2\nName: dataset_id, dtype: object\n","output_type":"stream"}]},{"cell_type":"code","source":"scores.to_csv('errors.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T00:20:29.781303Z","iopub.execute_input":"2024-04-05T00:20:29.781734Z","iopub.status.idle":"2024-04-05T00:20:29.790837Z","shell.execute_reply.started":"2024-04-05T00:20:29.781680Z","shell.execute_reply":"2024-04-05T00:20:29.789782Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}