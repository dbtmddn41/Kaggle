{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":67121,"databundleVersionId":7806901,"sourceType":"competition"},{"sourceId":169921578,"sourceType":"kernelVersion"},{"sourceId":11372,"sourceType":"modelInstanceVersion","modelInstanceId":5388,"modelId":3533}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dbtmddn41/prompt-recovery-keras-tpu-train?scriptVersionId=170113483\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"copied from https://www.kaggle.com/code/nilaychauhan/keras-gemma-distributed-finetuning-and-inference","metadata":{}},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"import datetime\n\nstart_time = datetime.datetime.now()","metadata":{"execution":{"iopub.status.busy":"2024-04-03T15:15:59.077273Z","iopub.execute_input":"2024-04-03T15:15:59.078208Z","iopub.status.idle":"2024-04-03T15:15:59.086746Z","shell.execute_reply.started":"2024-04-03T15:15:59.078115Z","shell.execute_reply":"2024-04-03T15:15:59.086112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    seed = 42\n    dataset_path = \"/kaggle/input/llm-prompt-recovery\"\n    preset = \"gemma_instruct_2b_en\" # name of pretrained Gemma\n    sequence_length = 1024 # max size of input sequence for training\n    train_batch = 4 # size of the input batch in training\n    validation_batch = 8\n    epochs = 4 # number of epochs to train\n    test_size = 0.\n    train_datas = ['kishanvavdara', 'newtonbaba12345_3', 'newtonbaba12345_1', 'aatiffraz', 'newtonbaba12345_2', 'host']\n    validation_datas = [\"winddude\"]\n    lora_rank=128\n    save_freq_steps = 1379","metadata":{"execution":{"iopub.status.busy":"2024-04-03T15:15:59.087951Z","iopub.execute_input":"2024-04-03T15:15:59.088251Z","iopub.status.idle":"2024-04-03T15:15:59.101332Z","shell.execute_reply.started":"2024-04-03T15:15:59.088218Z","shell.execute_reply":"2024-04-03T15:15:59.100785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n!pip install -q tensorflow-cpu\n!pip install -q -U keras-nlp tensorflow-hub\n!pip install -q -U keras>=3\n!pip install -U tensorflow-text\n!pip install parmap\n# !pip install -U jax jaxlib\n# !pip install -U sentence-transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-03T15:15:59.102088Z","iopub.execute_input":"2024-04-03T15:15:59.102304Z","iopub.status.idle":"2024-04-03T15:20:31.695701Z","shell.execute_reply.started":"2024-04-03T15:15:59.102282Z","shell.execute_reply":"2024-04-03T15:20:31.694629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import jax\n\njax.devices()","metadata":{"execution":{"iopub.status.busy":"2024-04-03T15:20:31.697875Z","iopub.execute_input":"2024-04-03T15:20:31.698175Z","iopub.status.idle":"2024-04-03T15:20:40.24138Z","shell.execute_reply.started":"2024-04-03T15:20:31.698145Z","shell.execute_reply":"2024-04-03T15:20:40.240576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# The Keras 3 distribution API is only implemented for the JAX backend for now\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n# Pre-allocate 90% of TPU memory to minimize memory fragmentation and allocation\n# overhead\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.0\"","metadata":{"execution":{"iopub.status.busy":"2024-04-03T15:20:40.242377Z","iopub.execute_input":"2024-04-03T15:20:40.242713Z","iopub.status.idle":"2024-04-03T15:20:40.246868Z","shell.execute_reply.started":"2024-04-03T15:20:40.242684Z","shell.execute_reply":"2024-04-03T15:20:40.246175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras\nimport keras_nlp\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-04-03T15:20:40.247763Z","iopub.execute_input":"2024-04-03T15:20:40.248011Z","iopub.status.idle":"2024-04-03T15:20:49.241926Z","shell.execute_reply.started":"2024-04-03T15:20:40.247985Z","shell.execute_reply":"2024-04-03T15:20:49.241092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a device mesh with (1, 8) shape so that the weights are sharded across\n# all 8 TPUs.\ndevice_mesh = keras.distribution.DeviceMesh(\n    (1, 8),\n    [\"batch\", \"model\"],\n    devices=keras.distribution.list_devices())","metadata":{"execution":{"iopub.status.busy":"2024-04-03T15:20:49.242848Z","iopub.execute_input":"2024-04-03T15:20:49.243291Z","iopub.status.idle":"2024-04-03T15:20:49.247575Z","shell.execute_reply.started":"2024-04-03T15:20:49.24326Z","shell.execute_reply":"2024-04-03T15:20:49.246939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_dim = \"model\"\n\nlayout_map = keras.distribution.LayoutMap(device_mesh)\n\n# Weights that match 'token_embedding/embeddings' will be sharded on 8 TPUs\nlayout_map[\"token_embedding/embeddings\"] = (None, model_dim)\n# Regex to match against the query, key and value matrices in the decoder\n# attention layers\nlayout_map[\"decoder_block.*attention.*(query|key|value).*kernel\"] = (\n    None, model_dim, None)\n\nlayout_map[\"decoder_block.*attention_output.*kernel\"] = (\n    None, None, model_dim)\nlayout_map[\"decoder_block.*ffw_gating.*kernel\"] = (model_dim, None)\nlayout_map[\"decoder_block.*ffw_linear.*kernel\"] = (None, model_dim)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T15:20:49.248471Z","iopub.execute_input":"2024-04-03T15:20:49.248706Z","iopub.status.idle":"2024-04-03T15:20:49.269769Z","shell.execute_reply.started":"2024-04-03T15:20:49.248681Z","shell.execute_reply":"2024-04-03T15:20:49.26914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keras.mixed_precision.set_global_policy(\"mixed_bfloat16\")","metadata":{"execution":{"iopub.status.busy":"2024-04-03T15:20:49.270646Z","iopub.execute_input":"2024-04-03T15:20:49.270875Z","iopub.status.idle":"2024-04-03T15:20:49.28361Z","shell.execute_reply.started":"2024-04-03T15:20:49.270851Z","shell.execute_reply":"2024-04-03T15:20:49.282899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"model_parallel = keras.distribution.ModelParallel(\n    device_mesh, layout_map, batch_dim_name=\"batch\")\n\nkeras.distribution.set_distribution(model_parallel)\ngemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(CFG.preset)\ngemma_lm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-03T15:20:49.286171Z","iopub.execute_input":"2024-04-03T15:20:49.286397Z","iopub.status.idle":"2024-04-03T15:21:42.002103Z","shell.execute_reply.started":"2024-04-03T15:20:49.286375Z","shell.execute_reply":"2024-04-03T15:21:42.001347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Datasets","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport gc","metadata":{"execution":{"iopub.status.busy":"2024-04-03T15:21:42.003099Z","iopub.execute_input":"2024-04-03T15:21:42.003361Z","iopub.status.idle":"2024-04-03T15:21:42.006812Z","shell.execute_reply.started":"2024-04-03T15:21:42.003335Z","shell.execute_reply":"2024-04-03T15:21:42.006171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndf = pd.read_csv(\"/kaggle/input/all-in-one-dataset-with-embedding/df_with_emb_20240402.csv\")\n\nfloat_cols = df.select_dtypes('float64').columns\ndf[float_cols] = df[float_cols].astype('float32')\ndf = df[df['rewritten_text'].str.len() >= 20]\ntrain_df = df[df['dataset_id'].isin(CFG.train_datas)].copy(deep=True)\nval_df = df[df['dataset_id'].isin(CFG.validation_datas)].sample(frac=0.02).copy(deep=True)\n\n# df = df.drop(index=0)\ngc.collect()\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-03T15:22:01.859393Z","iopub.execute_input":"2024-04-03T15:22:01.860289Z","iopub.status.idle":"2024-04-03T15:23:58.137433Z","shell.execute_reply.started":"2024-04-03T15:22:01.860251Z","shell.execute_reply":"2024-04-03T15:23:58.136675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()\nval_df.info()","metadata":{"execution":{"iopub.status.busy":"2024-04-03T15:24:19.430386Z","iopub.execute_input":"2024-04-03T15:24:19.430725Z","iopub.status.idle":"2024-04-03T15:24:19.559959Z","shell.execute_reply.started":"2024-04-03T15:24:19.430696Z","shell.execute_reply":"2024-04-03T15:24:19.559164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df[df.duplicated(subset=[\"rewrite_prompt\"], keep=False)]\ndisplay(train_df[[\"original_text\", \"rewrite_prompt\", \"rewritten_text\"]].nunique())\nprint('duplicated row:', train_df.duplicated(subset=[\"original_text\", \"rewrite_prompt\", \"rewritten_text\"], keep=False).sum())","metadata":{"execution":{"iopub.status.busy":"2024-04-03T15:24:19.561419Z","iopub.execute_input":"2024-04-03T15:24:19.561731Z","iopub.status.idle":"2024-04-03T15:24:19.61214Z","shell.execute_reply.started":"2024-04-03T15:24:19.561699Z","shell.execute_reply":"2024-04-03T15:24:19.611085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"각각은 중복된 text가 있지만 세 개 다 중복된 것은 없다.","metadata":{}},{"cell_type":"code","source":"import random\ndef display_random_row(df):\n    random_idx = random.randrange(0, len(df))\n    print(random_idx)\n    try:\n        print(\"\\033[38;2;255;0;0m\",df.loc[random_idx, [\"original_text\"]].values[0])\n        print(\"\\033[35m\t\", df.loc[random_idx, [\"rewrite_prompt\"]].values[0])\n        print(\"\\033[38;2;0;0;255m\", df.loc[random_idx, [\"rewritten_text\"]].values[0])\n        print('\\033[36m', df.loc[random_idx, [\"rewritte_prompt_predicted\"]].values[0])\n    except:\n        pass\ndisplay_random_row(train_df)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T15:24:19.61334Z","iopub.execute_input":"2024-04-03T15:24:19.613635Z","iopub.status.idle":"2024-04-03T15:24:19.6205Z","shell.execute_reply.started":"2024-04-03T15:24:19.613604Z","shell.execute_reply":"2024-04-03T15:24:19.61969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ndisplay(pd.concat([train_df[\"original_text\"].str.len(), train_df[\"rewrite_prompt\"].str.len(), train_df[\"rewritten_text\"].str.len()], axis=1).describe())\nbins = np.linspace(20, 2000, 50)\nplt.hist(train_df[\"original_text\"].str.len(), bins, alpha=0.5, label='original_text')\nplt.hist(train_df[\"rewrite_prompt\"].str.len(), bins, alpha=0.5, label='rewrite_prompt')\nplt.hist(train_df[\"rewritten_text\"].str.len(), bins, alpha=0.5, label='rewritten_text')\nplt.legend(loc=\"upper left\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-03T15:24:19.622485Z","iopub.execute_input":"2024-04-03T15:24:19.622776Z","iopub.status.idle":"2024-04-03T15:24:21.015228Z","shell.execute_reply.started":"2024-04-03T15:24:19.622745Z","shell.execute_reply":"2024-04-03T15:24:21.014255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def truncate_txt(text, length):\n    text_list = text.split()\n    \n    if len(text_list) <= length:\n        return text\n    \n    return \" \".join(text_list[:length])\n\n\ndef gen_val_prompt(df):\n    \n    # Truncate the texts to first 200 words for now\n    # As we are having memory issues on Mixtral8x7b\n    og_text = truncate_txt(df[\"original_text\"].strip(), CFG.sequence_length//3)\n    rewritten_text = truncate_txt(df[\"rewritten_text\"].strip(), CFG.sequence_length//3)\n    template = \"\"\"<bos>Instruct: Original Text:{}\\nRewritten Text:{}\\nWrite a prompt that was likely given to the LLM to rewrite original text into rewritten text. Output:\n<start_of_turn>model\n\"\"\"\n    return template.format(og_text, rewritten_text).strip()\n\ndef gen_prompt(df):\n    \n    # Truncate the texts to first 200 words for now\n    # As we are having memory issues on Mixtral8x7b\n    og_text = truncate_txt(df[\"original_text\"].strip(), CFG.sequence_length//3)\n    rewritten_text = truncate_txt(df[\"rewritten_text\"].strip(), CFG.sequence_length//3)\n    rewrite_prompt = truncate_txt(df[\"rewrite_prompt\"].strip(), CFG.sequence_length//3)\n    template = \"\"\"<bos>Instruct: Original Text:{}\\nRewritten Text:{}\\nWrite a prompt that was likely given to the LLM to rewrite original text into rewritten text. Output:\n<start_of_turn>model\n{}<end_of_turn><eos>\"\"\"\n    return template.format(og_text, rewritten_text, rewrite_prompt).strip()","metadata":{"execution":{"iopub.status.busy":"2024-04-03T15:24:21.01629Z","iopub.execute_input":"2024-04-03T15:24:21.016559Z","iopub.status.idle":"2024-04-03T15:24:21.023606Z","shell.execute_reply.started":"2024-04-03T15:24:21.016526Z","shell.execute_reply":"2024-04-03T15:24:21.022782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from multiprocessing import cpu_count, Pool\nfrom tqdm import tqdm\nimport parmap\n# tqdm.pandas()\ndef parallel_apply(df, main_func, func, n_cores=None):\n    if not n_cores:\n        n_cores = cpu_count()  # 사용 가능한 모든 CPU 코어를 사용\n\n    # 데이터를 코어 수만큼 분할\n    data_split = np.array_split(df, n_cores)\n    \n    # multiprocessing.Pool.map에 전달하기 위해 partial을 사용하여 함수 인자를 고정합니다.\n    from functools import partial\n    pool_func = partial(main_func, func=func)\n    \n#     pool = Pool(n_cores)\n    \n    # 각 코어에서 apply 함수를 실행\n    data = pd.concat(parmap.map(pool_func, data_split, pm_pbar=True, pm_processes=n_cores))\n#     pool.close()\n#     pool.join()\n    return data\n# apply 적용 함수\ndef apply_function(data, func):\n    return data.apply(func, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T15:24:21.024741Z","iopub.execute_input":"2024-04-03T15:24:21.024978Z","iopub.status.idle":"2024-04-03T15:24:21.046485Z","shell.execute_reply.started":"2024-04-03T15:24:21.024955Z","shell.execute_reply":"2024-04-03T15:24:21.045699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\ntqdm.pandas()\ntrain_df['prompt'] = train_df[[\"original_text\", \"rewritten_text\", \"rewrite_prompt\"]].progress_apply(gen_prompt, axis=1)\n\nval_df['val_prompt'] = val_df[[\"original_text\", \"rewritten_text\"]].progress_apply(gen_val_prompt, axis=1)\n\n# df['rewrite_prompt'] = df['rewrite_prompt'].progress_apply(lambda x: x.strip())\nif CFG.test_size > 0.:\n    train_df, val_df = train_test_split(df, test_size=CFG.test_size, random_state=42, stratify=df['dataset_id'])","metadata":{"execution":{"iopub.status.busy":"2024-04-03T15:24:21.0474Z","iopub.execute_input":"2024-04-03T15:24:21.047634Z","iopub.status.idle":"2024-04-03T15:24:22.158916Z","shell.execute_reply.started":"2024-04-03T15:24:21.047611Z","shell.execute_reply":"2024-04-03T15:24:22.157841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\ntrain_ds = tf.data.Dataset.from_tensor_slices(train_df['prompt'])\ntrain_ds = (train_ds\n            .batch(CFG.train_batch)\n            .shuffle(8192)\n            .prefetch(tf.data.AUTOTUNE)\n           )\n\nif val_df is not None:\n    val_ds = tf.data.Dataset.from_tensor_slices(val_df['val_prompt'])#{'val_prompt': val_df['val_prompt'], 'rewrite_prompt_emb': val_df.filter(like=\"rewrite_prompt_emb\").to_numpy()})\n    val_ds = (val_ds\n                .batch(CFG.validation_batch)\n                .prefetch(tf.data.AUTOTUNE)\n               )\n","metadata":{"execution":{"iopub.status.busy":"2024-04-03T15:24:22.160232Z","iopub.execute_input":"2024-04-03T15:24:22.161006Z","iopub.status.idle":"2024-04-03T15:24:22.19492Z","shell.execute_reply.started":"2024-04-03T15:24:22.160975Z","shell.execute_reply":"2024-04-03T15:24:22.193999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"# Enable LoRA for the model and set the LoRA rank to 4.\ngemma_lm.backbone.enable_lora(rank=CFG.lora_rank)\ngemma_lm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-03T15:24:22.195997Z","iopub.execute_input":"2024-04-03T15:24:22.196277Z","iopub.status.idle":"2024-04-03T15:24:23.027088Z","shell.execute_reply.started":"2024-04-03T15:24:22.196251Z","shell.execute_reply":"2024-04-03T15:24:23.026154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sentence_transformers import SentenceTransformer\n# from sklearn.metrics.pairwise import cosine_similarity\n\n# class ValidationMetricCallback(keras.callbacks.Callback):\n#     def on_epoch_end(self, epochs, logs):\n#         prompt_preds = self.model.generate(self.validation_data, max_length=512)\n        \nclass LoraCheckPointOnBatchs(keras.callbacks.Callback):\n    def __init__(self, save_freq_batchs):\n        super().__init__()\n        self.save_freq_batchs = save_freq_batchs\n        self.epochs = 0\n    def on_train_batch_end(self, batch, logs=None):\n        if batch % self.save_freq_batchs == 0 and batch != 0:\n            self.model.backbone.save_lora_weights(f\"{self.epochs}_{batch}-lora_weights.lora.h5\")\n    def on_epoch_end(self, epoch, logs=None):\n        self.epochs += 1\n\nclass LoraCheckPointOnBatchs(keras.callbacks.Callback):\n    def __init__(self, start_epochs=0):\n        super().__init__()\n        self.start_epochs = start_epochs\n    def on_epoch_end(self, epoch, logs=None):\n        if epoch >= self.start_epochs:\n            self.model.backbone.save_lora_weights(f\"{epoch}-lora_weights.lora.h5\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-03T15:24:23.029551Z","iopub.execute_input":"2024-04-03T15:24:23.029824Z","iopub.status.idle":"2024-04-03T15:24:23.036497Z","shell.execute_reply.started":"2024-04-03T15:24:23.029797Z","shell.execute_reply":"2024-04-03T15:24:23.035729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Limit the input sequence length to 512 (to control memory usage).\ngemma_lm.preprocessor.sequence_length = CFG.sequence_length \n\n# Compile the model with loss, optimizer, and metric\nlr = keras.optimizers.schedules.CosineDecay(\n    initial_learning_rate=8e-6,\n    decay_steps=10000//CFG.train_batch*10,\n    warmup_target=8e-5,\n    warmup_steps=3000//CFG.train_batch,\n)\ngemma_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=keras.optimizers.Adam(learning_rate=lr),\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n    sampler=\"greedy\"\n)\n# callbacks = [\n#     ValidationMetricCallback()\n# ]\n# Train model\ngemma_lm.fit(train_ds, epochs=CFG.epochs, callbacks=[LoraCheckPointOnBatchs(start_epochs=0)])","metadata":{"execution":{"iopub.status.busy":"2024-04-03T15:24:23.037387Z","iopub.execute_input":"2024-04-03T15:24:23.037622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test and save","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/llm-prompt-recovery/test.csv\")\ntemplate = \"\"\"<bos>Instruct: Original Text:{}\\nRewritten Text:{}\\nWrite a prompt that was likely given to the LLM to rewrite original text into rewritten text. Output:\n<start_of_turn>model\n\"\"\"\nprint(gemma_lm.generate(template.format(test_df.iloc[0,1], test_df.iloc[0,2]).strip(), max_length=512))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# final_norm_layer = gemma_lm.backbone.get_layer(\"final_normalization\")\ngemma_lm.backbone.save_lora_weights(\"final-lora_weights.lora.h5\")\n# gemma_lm.save_lora_weights(\"keras-gemma_instruct_7b_en-lora_weights.weights.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\njson_string = gemma_lm.to_json()\ndata = json.loads(json_string)\nwith open(\"config.json\", \"w\") as f:\n    json.dump(data, f)\n# json_string = gemma_lm.preprocessor.tokenizer.to_json()\n# data = json.loads(json_string)\n# with open(\"tokenizer.json\", \"w\") as f:\n#     json.dump(data, f)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nimport glob\nval_df.to_csv(\"validation.csv\")\n\nlora_weights = glob.glob('*-lora_weights.lora.h5')\nfor lora_weight in tqdm(lora_weights):\n    if (datetime.datetime.now() - start_time) > datetime.timedelta(hours=9, minutes=30):\n        continue\n    gemma_lm.backbone.load_lora_weights(lora_weight)\n    val_output = gemma_lm.generate(val_ds, max_length=CFG.sequence_length)\n    val_df['rewritte_prompt_predicted'] = val_output\n    val_df.to_csv(f\"{lora_weight.split('-')[0]}_validation.csv\", columns = [\"original_text\", \"rewrite_prompt\", \"rewritten_text\", \"rewritte_prompt_predicted\"],)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_df=val_df.reset_index()\ndisplay_random_row(val_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
