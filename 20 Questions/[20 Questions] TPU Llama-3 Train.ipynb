{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":8084888,"sourceType":"datasetVersion","datasetId":4750478},{"sourceId":186574388,"sourceType":"kernelVersion"},{"sourceId":33551,"sourceType":"modelInstanceVersion","modelInstanceId":28083,"modelId":39106}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"class CFG:\n    seed = 42\n    dataset_path = \"cvmistralparis/Q20LLM\"\n    preset = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"  # name of pretrained Gemma\n    sequence_length = 1024 # max size of input sequence for training\n    train_batch = 8 # size of the input batch in training\n    validation_batch = 16\n    lr = 0.001\n    epochs = 1 # number of epochs to train\n    lora_rank = 4\n    lora_alpha = 8\n    model_save_name = f'llama-3_lorarank-{lora_rank}_loraalpha{lora_alpha}_epoch{epochs}'","metadata":{"execution":{"iopub.status.busy":"2024-07-04T19:25:34.100395Z","iopub.execute_input":"2024-07-04T19:25:34.100840Z","iopub.status.idle":"2024-07-04T19:25:34.105910Z","shell.execute_reply.started":"2024-07-04T19:25:34.100804Z","shell.execute_reply":"2024-07-04T19:25:34.105102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip3 install transformers datasets peft -Uq\n# !pip install torch~=2.1.0 --index-url https://download.pytorch.org/whl/cpu -q # Updating torch since we need the latest version\n# !pip install torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html -q\n!pip uninstall tensorflow -y # If we don't do this, TF will take over TPU and cause permission error for PT\n!cp /kaggle/input/utils-xla/spmd_util.py . # From this repo: https://github.com/HeegyuKim/torch-xla-SPMD","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-04T19:21:14.740975Z","iopub.execute_input":"2024-07-04T19:21:14.741356Z","iopub.status.idle":"2024-07-04T19:21:19.474227Z","shell.execute_reply.started":"2024-07-04T19:21:14.741328Z","shell.execute_reply":"2024-07-04T19:21:19.472876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport datasets\nimport torch.optim as optim\nimport torch_xla.debug.profiler as xp\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp # We also import mp modules if we wanna use that for some reason\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.test.test_utils as test_utils\nimport torch\nimport torch.nn as nn\nimport re\nimport torch_xla.experimental.xla_sharding as xs\nimport torch_xla.core.xla_model as xm\nfrom transformers import (\n    AutoTokenizer, AutoModelForCausalLM, DataCollatorWithPadding, AutoConfig\n) # You can use any of models with those configs (even flan T5 xxl!). Other models are not supported.\n\nfrom transformers import logging as hf_logging\nimport torch.nn.functional as F\nimport torch_xla.runtime as xr\n\nxr.use_spmd()\n\nimport torch_xla.experimental.xla_sharding as xs # \"experimental\" prefix always means you're gonna have a good time LMAO\nfrom torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor\nfrom torch_xla.experimental.xla_sharding import Mesh\n\nfrom peft import LoraConfig, TaskType, get_peft_model # If we wanna use peft. Quantazation requiers GPU though. You'll have to download already quantazed models\nfrom spmd_util import partition_module                # You could experiment with using already quantazed models like 4bit/Llama-2-7b-Chat-GPTQ if you're feeling funny\nfrom datasets import Dataset, load_dataset, concatenate_datasets\nfrom dataclasses import dataclass\nfrom tqdm import tqdm\n\nfrom peft import LoftQConfig, LoraConfig, get_peft_model\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig\nimport datasets\nimport pandas as pd\nimport numpy as np\nfrom datasets import Dataset\nfrom sklearn.metrics import roc_auc_score\n\n!export USE_TORCH=True #If we don't do this, transformers will seemingly bork the session upon import. Really weird error.\nos.environ[\"PJRT_DEVICE\"] = \"TPU\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n# os.environ.pop('TPU_PROCESS_ADDRESSES')\n# os.environ.pop('CLOUD_TPU_TASK_ID')\nhf_logging.set_verbosity_error() # It can still display warnings which is a bit annoying but whatever\n\n\n# MAX_INPUT=512\n# MODEL = \"/kaggle/input/gemma/transformers/7b-it/2\" #You should be able to use 13B model with no changes! There should be enough HBM\ndevice = xm.xla_device()","metadata":{"execution":{"iopub.status.busy":"2024-07-04T19:21:45.153232Z","iopub.execute_input":"2024-07-04T19:21:45.153614Z","iopub.status.idle":"2024-07-04T19:21:45.296151Z","shell.execute_reply.started":"2024-07-04T19:21:45.153579Z","shell.execute_reply":"2024-07-04T19:21:45.294971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(CFG.preset)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\nmodel = AutoModelForCausalLM.from_pretrained(CFG.preset, torch_dtype=torch.bfloat16)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T19:21:50.363073Z","iopub.execute_input":"2024-07-04T19:21:50.363560Z","iopub.status.idle":"2024-07-04T19:22:15.499471Z","shell.execute_reply.started":"2024-07-04T19:21:50.363511Z","shell.execute_reply":"2024-07-04T19:22:15.498580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=CFG.lora_rank,\n    lora_alpha=CFG.lora_alpha,\n    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    task_type=\"CAUSAL_LM\",\n    lora_dropout=0.05,\n    bias=\"none\"\n#     inference_mode=True\n)\npeft_model = get_peft_model(model, lora_config)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T19:22:15.501189Z","iopub.execute_input":"2024-07-04T19:22:15.501500Z","iopub.status.idle":"2024-07-04T19:22:15.838260Z","shell.execute_reply.started":"2024-07-04T19:22:15.501469Z","shell.execute_reply":"2024-07-04T19:22:15.837423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = AutoConfig.from_pretrained(CFG.preset)\nnum_devices = xr.global_runtime_device_count()\nprint(num_devices)\nmesh_shape = (1, num_devices, 1, 1)\ndevice_ids = np.array(range(num_devices))\nmesh = Mesh(device_ids, mesh_shape, ('dp', 'fsdp', 'mp', 'sp'))\npartition_module(peft_model, mesh) # After this, the model is sharded between cores but still has the same API as if it was on single device. Neat.","metadata":{"execution":{"iopub.status.busy":"2024-07-04T19:22:15.839312Z","iopub.execute_input":"2024-07-04T19:22:15.839603Z","iopub.status.idle":"2024-07-04T19:22:29.485853Z","shell.execute_reply.started":"2024-07-04T19:22:15.839556Z","shell.execute_reply":"2024-07-04T19:22:29.484734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peft_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-07-04T19:22:29.487563Z","iopub.execute_input":"2024-07-04T19:22:29.487972Z","iopub.status.idle":"2024-07-04T19:22:29.498967Z","shell.execute_reply.started":"2024-07-04T19:22:29.487940Z","shell.execute_reply":"2024-07-04T19:22:29.498305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# datasets","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ntrain_df = pd.read_csv(\"/kaggle/input/20-questions-dataset/train.csv\")\nval_df = pd.read_csv(\"/kaggle/input/20-questions-dataset/validation.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-07-05T16:08:56.876221Z","iopub.execute_input":"2024-07-05T16:08:56.876887Z","iopub.status.idle":"2024-07-05T16:08:57.984306Z","shell.execute_reply.started":"2024-07-05T16:08:56.876850Z","shell.execute_reply":"2024-07-05T16:08:57.983090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import Iterable\nimport itertools\n\nclass LlamaFormatter:\n    _bos_token = '<|begin_of_text|>'\n    _start_header_token = '<|start_header_id|>'\n    _end_header_token = '<|end_header_id|>'\n    _end_token = '<|eot_id|>'\n\n    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n        self._system_prompt = system_prompt\n        self._few_shot_examples = few_shot_examples\n        self._turn_system = f\"{self._start_header_token}system{self._end_header_token}\\n\\n{{}}{self._end_token}\"\n        self._turn_user = f\"{self._start_header_token}user{self._end_header_token}\\n\\n{{}}{self._end_token}\"\n        self._turn_model = f\"{self._start_header_token}assistant{self._end_header_token}\\n\\n{{}}{self._end_token}\"\n        self.reset()\n\n    def __repr__(self):\n        return self._state\n    \n    def system(self, prompt):\n        self._state += self._turn_system.format(prompt)\n        return self\n    \n    def user(self, prompt):\n        self._state += self._turn_user.format(prompt)\n        return self\n\n    def model(self, prompt):\n        self._state += self._turn_model.format(prompt)\n        return self\n\n    def start_user_turn(self):\n        self._state += f\"{self._start_header_token}user{self._end_header_token}\\n\\n\"\n        return self\n\n    def start_model_turn(self):\n        self._state += f\"{self._start_header_token}assistant{self._end_header_token}\\n\\n\"\n        return self\n\n    def end_turn(self):\n        self._state += f\"{self._end_token}\\n\"\n        return self\n\n    def reset(self):\n        self._state = \"\"\n        self._state += self._bos_token\n        if self._system_prompt is not None:\n            self.system(self._system_prompt)\n        if self._few_shot_examples is not None:\n            self.apply_turns(self._few_shot_examples, start_agent='user')\n        return self\n\n    def apply_turns(self, turns: Iterable, start_agent: str):\n        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n        formatters = itertools.cycle(formatters)\n        for fmt, turn in zip(formatters, turns):\n            fmt(turn)\n        return self","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-07-05T16:19:09.792250Z","iopub.execute_input":"2024-07-05T16:19:09.792656Z","iopub.status.idle":"2024-07-05T16:19:09.804904Z","shell.execute_reply.started":"2024-07-05T16:19:09.792626Z","shell.execute_reply":"2024-07-05T16:19:09.803749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nfrom tqdm import tqdm\n\ndef get_random_word(exclude_word=''):\n    w = random.choice(words['train']['keyword'])\n    while w == exclude_word:\n        w = random.choice(words['train']['keyword'])\n    return w\n\nsystem_prompt = \"You are an AI assistant designed to play the 20 Questions game. In this game, the Answerer thinks of a keyword and responds to yes-or-no questions by the Questioner. The keyword is a specific places or things.\"\nprompts = []\nrear_keyword = ''\nkeyword_cnt = 1\nfor row in tqdm(train_df.itertuples(index=False), total=len(train_df)):\n    if rear_keyword == row.label:\n        keyword_cnt += 1\n    else:\n        keyword_cnt = 1\n        rear_keyword = row.label\n    formatter = LlamaFormatter(system_prompt=system_prompt)\n    formatter.user(\"Let's play 20 Questions. You are playing the role of the Questioner. The keyword is a specific places or things.\")\n    prompt = eval(row.prompt)\n    formatter.apply_turns(turns=prompt, start_agent='model')\n    formatter.user('Now guess the keyword.')\n    if keyword_cnt % 100 != 0:\n        formatter.model('**'+row.label+'**')\n        formatter.user('Correct!')\n    else:\n        formatter.model('**'+get_random_word(row.label)+'**')\n        formatter.user('Wrong!')\n  \n    prompts.append(formatter._state)\n    del formatter\n","metadata":{"execution":{"iopub.status.busy":"2024-07-05T16:19:14.274719Z","iopub.execute_input":"2024-07-05T16:19:14.275143Z","iopub.status.idle":"2024-07-05T16:19:14.773832Z","shell.execute_reply.started":"2024-07-05T16:19:14.275108Z","shell.execute_reply":"2024-07-05T16:19:14.772849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\n\ndef preprocess_func(example):\n    inputs = tokenizer(example['text'], truncation=True, max_length=CFG.sequence_length, padding='max_length')\n    return (\n    {\n        'input_ids': inputs.input_ids,\n        'attention_mask': inputs.attention_mask\n    })\n\ntrain_ds = Dataset.from_dict({\"text\": prompts})\ntrain_ds = train_ds.map(preprocess_func)\ntrain_ds = train_ds.remove_columns([\"text\"])","metadata":{"execution":{"iopub.status.busy":"2024-07-05T16:19:14.775544Z","iopub.execute_input":"2024-07-05T16:19:14.775857Z","iopub.status.idle":"2024-07-05T16:19:15.090071Z","shell.execute_reply.started":"2024-07-05T16:19:14.775832Z","shell.execute_reply":"2024-07-05T16:19:15.088463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import DataCollatorWithPadding\ntraindata_loader = torch.utils.data.DataLoader(train_ds, batch_size=CFG.train_batch, collate_fn=DataCollatorWithPadding(tokenizer=tokenizer),shuffle=True, num_workers=8)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T16:09:07.920980Z","iopub.status.idle":"2024-07-05T16:09:07.921429Z","shell.execute_reply.started":"2024-07-05T16:09:07.921243Z","shell.execute_reply":"2024-07-05T16:09:07.921260Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(prompts[0])","metadata":{"execution":{"iopub.status.busy":"2024-07-05T16:19:20.776694Z","iopub.execute_input":"2024-07-05T16:19:20.777254Z","iopub.status.idle":"2024-07-05T16:19:20.783197Z","shell.execute_reply.started":"2024-07-05T16:19:20.777214Z","shell.execute_reply":"2024-07-05T16:19:20.782139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"!export XLA_USE_BF16=1\ndef train(\n    model, train_data, validation_data=None, train_batch=4, validation_batch=8, epochs=10, logging_steps=1,\n    lr=1e-5, \n):\n#     def train_model(model, input_ids, attention_mask, optimizer):\n#         output = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n#         loss = output.loss\n#         loss.backward()\n#         optimizer.step()\n#         return loss\n    \n#     compiled_step = torch.compile(model, backend=\"openxla\")\n    \n    optimizer = optim.AdamW(model.parameters(), lr=lr)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=6000/train_batch*epochs)\n    for epoch in range(1, epochs + 1):\n        model.train()\n        xm.master_print('Epoch {} train begin {}'.format(epoch, test_utils.now()))\n        pbar = tqdm(range(len(train_data)))#, disable=True)\n        data_iter = iter(train_data)\n        total_loss = 0.\n#         with torch.autocast(\"xla\", dtype=torch.bfloat16):\n        for step, batch in enumerate(pbar):\n            batch = next(data_iter)\n            optimizer.zero_grad()\n            input_ids, attention_mask = batch.input_ids.to(device), batch.attention_mask.to(device)\n            xs.mark_sharding(input_ids, mesh, (0, 1)) # Sharding inputs\n            xs.mark_sharding(attention_mask, mesh, (0, 1))\n\n#                 loss = compiled_step(model, input_ids, attention_mask, optimizer)\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n            xm.mark_step()\n            total_loss = (total_loss * step + loss.item()) / (step + 1)\n            if (step + 1) % logging_steps == 0:\n                pbar.set_postfix({'loss': total_loss, 'step': step+1, 'epoch': epoch})\n            scheduler.step()\n#             if step > 10:\n#                 break\n        xm.master_print('Epoch {} train end {}, loss={:.3f}'.format(epoch, test_utils.now(), total_loss))\n        model.eval()\n        total_val_loss = 0.0\n        if validation_data is not None:\n            pbar = tqdm(range(len(validation_data)))#, disable=True)\n            data_iter = iter(validation_data)\n            with torch.no_grad():\n                for step, batch in enumerate(pbar):\n                    batch = next(data_iter)\n                    input_ids, attention_mask = batch.input_ids.to(device), batch.attention_mask.to(device)\n                    xs.mark_sharding(input_ids, mesh, (0, 1))\n                    xs.mark_sharding(attention_mask, mesh, (0, 1))\n                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n                    loss = outputs.loss\n                    total_val_loss = (total_val_loss * step + loss.item()) / (step + 1)\n                    if (step + 1) % logging_steps == 0:\n                        pbar.set_postfix({'val_loss': total_val_loss, 'step': step+1, 'epoch': epoch})\n\n            xm.master_print('Epoch {} test end {}, test val_loss={:.3f}'.format(epoch, test_utils.now(), total_val_loss))\n        \n\n        \n    model.cpu()\n    model.save_pretrained(CFG.model_save_name)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T19:23:27.059321Z","iopub.execute_input":"2024-07-04T19:23:27.059621Z","iopub.status.idle":"2024-07-04T19:23:27.839394Z","shell.execute_reply.started":"2024-07-04T19:23:27.059594Z","shell.execute_reply":"2024-07-04T19:23:27.838020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train(peft_model, traindata_loader, train_batch=CFG.train_batch, epochs=CFG.epochs, lr=CFG.lr)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T19:25:38.498550Z","iopub.execute_input":"2024-07-04T19:25:38.498957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
